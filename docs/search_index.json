[["index.html", "Utilisation_cluster_de_calcul My personal courses Preface", " Utilisation_cluster_de_calcul My personal courses Vincyane 2025-04-17 Preface This book contains my personal courses. Some are in French, some in English, or both…sorry. Have a nice reading. Vincyane "],["introduction.html", "Introduction", " Introduction Blah. "],["fonctions-bien-sympatiques-tres-cher.html", "Cours 1 Fonctions bien sympatiques tres cher 1.1 Conseils 1.2 Tips de codeur 1.3 Packages inédits! 1.4 Mes raccourcis claviers 1.5 Symbol/name, charactères, variable/objet tout le bazar 1.6 Charger les données 1.7 Exportation des données 1.8 Correction préliminaires des données 1.9 Transformation de données ! 1.10 Histoire de characters 1.11 Comparaison de tables 1.12 Tirage 1.13 Yo ! arrange mes données ! 1.14 Donne moi des infos sur mes données 1.15 Calcul informatif 1.16 Nettoyage 1.17 Boucle 1.18 Les objets 1.19 Condition 1.20 Logical Operators 1.21 Stan - Modelo - Bayes 1.22 Faire de la biblio comme une déesse 1.23 Date &amp; time", " Cours 1 Fonctions bien sympatiques tres cher Description : du R base, des fonctions de “base”, du tidyverse. Pour du data.table voir le cours concerné. ICI JE METS JUSTE LE NOM DES FONCTIONS, MAIS FAUT CHERCHER SUR INTERNET/LES PAGES D’AIDE LES MODALITES D’UTILISATION !! 1.1 Conseils Eviter de donner le meme nom au pipe que la base qu’on utilise dedans, ça écrase les données, et c’est la galere pour les reccuperer Les fonctions tidy sont pas faites pour s’imbriquer. Utiliser des fonctions base à l’int des fcts tidy. Qd un prblm est trop gros, découper le travail (ex: faire des sous-tables) 1.2 Tips de codeur Selectionner tout ce qu’il ya dans la parenthèse/crochet/accolade : double clic à l’interieur de la 1ère/dernière Mesurer le temps d’exécution d’un code : system.time() ou microbenchmark (fct1,fct2) pour des codes trés courts. Ce sont les médianes des temps de calcul qu’il faut comparer. start.time &lt;- Sys.time() # fonction appelée ici end.time &lt;- Sys.time() time.taken &lt;- end.time - start.time time.taken détacher un package de l’environnement : unloadNamespace() connaitre le code d’une fct : trace(pkgname::fct, edit=T) (obt &lt;- code) : affiche l’objet Empecher l’affichage d’un message d’erreur : try( , silent=TRUE) # no error printed Progress bar : pb = txtProgressBar(min = 0, max = length(ind), style = 3) then close(pb) debuger : traceback() Tracer un warning : options(warn = 2) (le transforme en erreur) quand fin de debuger options(warn = 0) mettre au propre un code (réindenter, -&gt;, espaces, taille des lignes) : package “styler” &gt; Addins &gt; Style active file créer une table de combinaisons de paramètres : tidyr::crossing(a, b, c) 1.3 Packages inédits! Shiny pour trouver des packages R dans le milieu forestier : ForestAnalysisInR::launchRFA() 1.4 Mes raccourcis claviers Tout selectionner : Ctrl + A Sélection de texte : maj + flèche dans sens de selection (on peut délimiter avec clic souris) Indenter : Ctrl + I Recherche sur toutes les pages du projet : Ctrl + Shift + F (on peut faire du replace !) %&gt;% : Ctrl + Shift + M %in% : Ctrl + Shift + I (nécéssite un package) Install the package: devtools::install_github(“rstudio/addinexamples”, type = “source”) Restart R 1.5 Symbol/name, charactères, variable/objet tout le bazar nom d’objet -&gt; ce nom en characteres : deparse(substitute(var)) chaine de characteres -&gt; nom d’objet : as.name(“name”) chaine de characteres -&gt; objet : get(“var”) ou {{var}} ou … (https://dplyr.tidyverse.org/articles/programming.html) : my_summarise &lt;- function(.data, ...) { .data %&gt;% group_by(...) %&gt;% summarise(mass = mean(mass, na.rm = TRUE), height = mean(height, na.rm = TRUE)) } starwars %&gt;% my_summarise(sex, gender) 1.6 Charger les données Chemin absolu : écriture de l’entièreté du chemin Chemin relatif : ~ = le dossier du setwd ./ = là où je suis Les .rmd ont leur propre racine, donc pas celle de leur projet. ” ../ = dossier parent (du niv de dessus) et je mets autant de ../ que de niveau à remonter. Puis tabulation pour qu’il propose mes fichiers. Stocker mon chemin relatif dans un objet « path ». Pour lire mon chemin stocké : file.path(path, “dossier”, “file”) Pour ouvrir une fenetre pour choisir le fichier : nomObjt &lt;- file.choose() !!!!!!Environment &gt; import dataset &gt; tjrs spécifier le séparateur (delimiter) et le marqueur de décimale (locale &gt; decimal mark)!!!!!!!!! (franchement ça fait trop de la m**rd sinon) Importer avec les accents : locale = locale(encoding = “latin1”) Importer tous les fichiers d’un dossier : filenames &lt;- list.files(path, pattern=&quot;*.csv&quot;, full.names=TRUE) # catch the name of all the files of the folder df_list &lt;- lapply(filenames, read.csv, header=FALSE, sep=&quot;;&quot;) # read all the folder files lire du texte qui n’est pas une table : readLines() Enregistrer 1.7 Exportation des données en csv : write.csv(obj, “path/data.txt”) en fichier texte : write.table(obj, “path/data.txt”,sep=“;”) 1 object en r data : saveRDS(obj, file = “./folder/filename.rds”) puis objname &lt;- readRDS(“./folder/filename.rds”) plsrs objets : save(obj1, obj2, file = “./folder/filename.Rdata”) puis load(“./folder/filename.Rdata”) tout l’environnement : save.image(file = “my_work_space.RData”) puis load(“my_work_space.RData”) un renvoi consol : sink(&quot;D:/Mes Donnees/PhD/Light/LAI-2200/DATA/Transmittance_cor.txt&quot;) # path objt sink() # close the external connection en fichier Excel : **write.table(obj,“data.xls”,sep=“,row.names = F, dec=”,“)** il faut une tabulation”/t Importer du shapefile (shp) : rgdal::readORG(dsn = “/path/to/your/file”, layer = “filename”) ou raster::shapefile(“path/to/your/file.shp”) 1.8 Correction préliminaires des données Corriger les arbres non touvés mais vivants dans inventaires : package forestdata sur github ecofog 1.9 Transformation de données ! Supprimer une colonne/ligne vide : janitor::remove_empty(data,“cols”) Renomer une variable : rename(New = Old) Renames columns using a function (here add a suffix): **rename_with(.fn = ~paste0(., “_suffix”), .cols = c(“a”, “b”, “c”))** De character à factor : mutate(Vernacular = factor(as.character(Vernacular))) Séparer 1 colonne en plsrs : **separate(data, col à séparer, sep = “_“, into = c(nom des new col), remove = F)** remove = F pour conserver la colonne qu’on sépare Combiner des colonnes en une : unite(col1, col2, col = “sp”, sep = ” “, remove = T) La même qd ya pas de séparateur : mutate(variable = substr(x, start, stop)) ou StrLeft(x, n) StrRight(x, n) en partant de la fin. -n pour enlever n paramètre Recoder une variable : mutate(var = recode(var, “old1” = “new1” , “old2” = “new2”)) le nouveau code derière le = si ya erreur essayer : “‘1’ = ‘A’ ; ‘2’ = ‘B’” guillemets double et simple et point virgule Changer une variable sous condition : mutate(var = ifelse(condition, si oui, sinon)) Sous plusieurs conditions : séparer les conditions d’un &amp; pour faire plusieurs ifelse pour le calcul d’une variable : mutate(var = case_when( x == 1 ~ &quot;A&quot;, x == 10 ~ &quot;B&quot;, x == 20 ~ &quot;C&quot;, .default = var) # le reste reste comme c&#39;est ) Changer le nombre de décimales : format(x, digits = 0, scientific = F) ou round(x, digits = 0) Arrondir tout un df sur uniquement les variables numériques : mutate_if(is.numeric, round, digits = 2) effectuer par ligne : rowwise() Remplir une colonne à partir d’une fonction dont l’input est une table : do() déplie la une sous-table existant dans une table : unnest() Encode a column in utf8 : tidyft::utf8_encoding(datatable, colname) Mettre à NA 10% du jeu de donnée aléatoirement : data[sample(nrow(data), 0.1 * nrow(data)), c(“X”, “Y”, “Z”) := NA_real_] Calculate row means on subset of columns : mutate(Mean = rowMeans(select(., var1, var2))) Standardiser pour sommer à 100% : **var/sum(Sum))*100** joindre des tables : **left_join(dataB, by= c(“var1”, “var2”), suffix = c(“_A”, “_B”)))** Replace each NA with the most recent non-NA prior to it : zoo::na.locf() prendre toutes les colones : everything() 1.10 Histoire de characters 1.10.1 Create paste(val, ” truc”, val, “truc”, “”, sep = “,”) 1.10.2 Detect Detect special characters (logical) : x &lt;- “!” grepl(‘[[:punct:]]’, x) # TRUE if there are any special character, FALSE if not. Special pattern : grepl(“pattern”, var) dot/point can be “\\.” 1.10.3 Extract Extraire un element numeric dans une chaîne de characters : readr::parse_number(var) ou gsub(“[[:digit:]]”, ““, var) Extraire un élément d’une chaine de characters : substring(ID, first=1, last=2) si data\\(idtree = &quot;site_plot&quot; **data\\)site = data.table::tstrsplit(data\\(idtree, split = &quot;_&quot;)[[1]]** 1er elmt **data\\)plot = data.table::tstrsplit(data$idtree, split = “_“)[[2]]** 2nd 1.10.4 Convert Mettre le nom d’un objet sous forme de characters : deparse(substitute(obj)) Afficher un data.frame en characters : a &lt;- data.frame(x=runif(4), y=runif(4), z=runif(4)) b &lt;- capture.output(a) c &lt;- paste(b, &quot;\\n&quot;, sep=&quot;&quot;) cat(&quot;Your data set is:\\n&quot;, c, &quot;\\n&quot;) Mettre en minuscule/majuscule : tolower()/toupper() 1ère lettre de l’expression en majuscule : str_to_sentence() Remplacer un pattern : str_replace(df$address, “St”, “Street”) 1.10.5 Remove Remove specific pattern : str_remove(string, pattern) Remove rows with a special pattern : filter(!grepl(“pattern”, var)) Remove whitespace : gsub(“[[:space:]]”, ““, x) removes whitespace at the start and end, and replaces all internal whitespace with a single space : str_squish() Remove parentheses : gsub(“[()]”, ““, my_string) Remove brackets : gsub(“\\[|\\]”, ““, string) Also remove text within () : **gsub(“\\s*\\([^\\\\)]+\\)“,”“, my_string)** Remove non-breaking space : gsub(“[^ -~]+”,““, x)) Remove special characters : x &lt;- “a-,_b/” **gsub(“[[:punct:]]”, ““, x)** x la chaine de characters à modifier, entre” ” mettre par quoi remplacer les special charcters enlevés (ou ne rien mettre) Enlever un espace avant et/ou apres un character : trimws(x, which = c(“both”, “left”, “right”)) Enlever accent : sub(“é”,“e”,vector)) 1.11 Comparaison de tables diffdf(,) 1.12 Tirage Tirer aléatoirement n individus sans remplacement : sample_n(n, replace = F) 1.13 Yo ! arrange mes données ! ## Quand tu veux passer une collone de df en argument de fonction : ## V0 column_name &lt;- &quot;Position&quot; ## character string in an object column &lt;- inventory[,column_name] ## character vector line_nonna &lt;- which(!is.na(column)) ## which rows different of NA subinventory &lt;- inventory[line_nonna,] ## filter base version ## V1 column_name &lt;- &quot;Position&quot; column &lt;- inventory[,column_name] nonna &lt;- !is.na(column) ## logical vector subinventory &lt;- inventory[nonna,] ## take only the TRUE ## V2 column_name &lt;- &quot;Position&quot; inventory[!is.na(inventory[,column_name]),] %&gt;% st_as_sf(wkt = column_name) ## Vdplyr col_name &lt;- deparse(substitute(var)) ## object name to this name in character(get() pour faire l&#39;inverse) inventory %&gt;% filter(!is.na( {{ var }} )) %&gt;% ## {{var}} pour réccupérer l&#39;objet mis en argument &quot;var&quot; st_as_sf(wkt = col_name) ## quand t&#39;as besoin du nom de la colonne en charactère #ou modif_var &lt;- function(data, nom_variable){ data %&gt;% dplyr::mutate(nouveau_nom = !!rlang::sym(nom_variable)+1) } Inverser lignes &amp; colonnes (retourner) (transposer) : t() Mettre une colonne en colonne “rownames” : column_to_rownames(data, varàdéplacer) Mettre “rownames” en colonne : tibble::rownames_to_column(“colname”) Mettre l’information de plusieurs colonnes en 1 : melt() Inverse : dcast(col1 + col2 ~ newcol) Ne prendre qu’une partie des données : filter() ou subset() Si on veut garder les NA, comme filter les vire d’office, rajouter | is.na(var) Pour filtrer beaucoup de valeurs d’une même var: values &lt;- c(“Tom”, “Lynn”) vecteur de n’importe quoi filter(dat, var %in% values) Ne pas prendre une partie des données (valeurs) : filter(!(Vernacular %in% remove)) (enlever remove dans vernacular) Quand dans un booléen les Na dérangent, utiliser plutot %in% que == (ex: a %in% TRUE) Enlever des colonnes : select(-colname) Réaliser les opérations suivantes par groupes de modalités : group_by Créer une variable résumant les effectifs des éléments d’une variable : summarise(N = n()) ou count(sp, sort = T,name = “N”) ou group_by(var) %&gt;% mutate(N = n()) %&gt;% ungroup() pour la mettre dans le df initial. Calculer des stats de base par variable, et variables groupées : HOBOdata %&gt;% group_by(HoboID, Phase) %&gt;% # variables selon lesquelles grouper summarise(across(c(&quot;Temp&quot;, &quot;Light&quot;), # Variables à analyser list(mean = mean, # les stats sd = sd, max = max, min = min))) # si ya une variable qui a besoin d&#39;autres variables : phase_stats &lt;- HOBOdata %&gt;% group_by(HoboID, Phase) %&gt;% summarise(across(c(&quot;Temp&quot;, &quot;Light&quot;), list(mean = ~mean(.), sd = ~sd(.), VarCoef = ~(sd(.) / mean(.)) * 100, # la var qui en utilise d&#39;autres max = ~max(.), min = ~min(.)), .names = &quot;{.col}_{.fn}&quot;)) # &quot;Phase&quot; variable as an index for the computed variables phase_stats &lt;- pivot_wider(phase_stats, names_from = Phase, values_from = names(phase_stats)[-c(1:2)], names_glue = &quot;{.value}_{Phase}&quot;) par ordre croissant (pour un vecteur!) : sort(x, decreasing = FALSE, na.last = NA, …) ou order() Pour une col dans un dataframe : Effectifs par ordre croissant : arrange(N) Effectifs par ordre décroissant : arrange(desc(N)) Ordonner des chiffres en character : gtools::mixedsort(sort(filenames)) Ranger par ordre alphabétique : d’abord transformer la variable character en factor, puis arrange(var) Selectionner les n valeurs maximales : top_n Afficher les 30 1ères lignes : slice(1:30) Changer ordre des colonnes : select(c(a, y, x, 1ercolnonmodified : dercol)) Séparer une base de données, selon un facteur, en une liste de bases de données : split(data, data$fact) Enlever les lignes en plusieurs exemplaires/connaitre les valeurs que peut prendre une colonnne : unique() ou que sur une variable : distinct(idTree, .keep_all = T) Savoir si ttes les valeurs d’une colonne sont les mêmes: length(unique(data\\(var)) == 1** ou **all(data\\)var == data$var[1]) = est-ce que toutes les valeurs sont égales à la 1ère Répliquer une ligne n fois : do.call(“rbind”, replicate(n, df, simplify = FALSE)) Populer les valeurs (si NA) pour une var groupée : group_by(TreeID) %&gt;% fill(X, Guyafor.nb, ScientificName, .direction = “downup”) Joindre 2 tables : left_join(df1, df2, by = &#39;key&#39;, suffix = c(&quot;_1&quot;, &quot;_2&quot;)) %&gt;% # by = c(&#39;coldf1&#39; = &#39;coldf2&#39;)) if not the same name mutate( # mutate sur plusieurs variables var1 = coalesce(var1.x, var1.y), # pour combiner les valeurs de .x et .y var2 = coalesce(var2.x, var2.y) ) %&gt;% select(-(contains(&quot;.x&quot;) | contains(&quot;.y&quot;))) Ne garder les lignes que sans NA dans n’importe quelle colonne : na.omit(df) ou df[!complete.cases(df), ] Removing Rows with Only NAs : data[rowSums(is.na(data)) != ncol(data), ] ou filter(data, rowSums(is.na(data)) != ncol(data)) 1.14 Donne moi des infos sur mes données pH[pH&gt;7] : afficher les ph&gt;7 length(pH[pH&gt;7]) : combien de ph&gt;7 data[pH&gt;7,] : afficher les lignes dont les ph&gt;7 which(x==a) : renvoie les indices (n°de lignes) de x pour lesquels le résultat de l’opération logique est vrai Indicer selon une modalité : data[data$var ==“modalite”,] Nombre de NA dans 1 colonne : sum(is.na(data$x_num)) quelles colonne ont des NA : names(df)[sapply(df, anyNA)] Nombre de lignes ayant pour valeur “truc” dans telle variable sum(as.numeric(data$var == “truc”), na.rm = TRUE) pcq as.numeric transforme un vecteur de booléen en vecteur de 0 et 1. Nombre de NA dans le jeu de de données, dans chaque colonne : colSums(is.na(data)) est-ce que tt ça est vrai : all() au moins 1 vrai : any() ne prend pas de liste (enfin si mais il n’est pas content)! renvoyer les éléments dupliqué : duplicated() (booléen) data[duplicated(data$var), ] (dataframe) anyDuplicated Générer la numérotation/les id des lignes : seq.int(nrow(df)) Nombre pair/impair : var %% 2 == F 1.15 Calcul informatif Faire la somme/moyenne/autre de chaque ligne/colonne : apply(data, c(1,2), sum) 1 : par ligne ou 2 : par colonne Appliquer une fonction sur une liste/vecteur : lapply(liste, function(elementdelaliste) la fonction dépendante de l’élement) ou lapply(liste, function, autres arguments de la fct) https://www.datacamp.com/tutorial/r-tutorial-apply-family ex : multiply &lt;- function(x, factor, div) { (x * factor)/div } lapply(list(1,2,3), multiply, x = 5, factor = 3) # the list is for the &quot;div&quot; argument lapply(a, function(element) bcDiversity(element$N, q = 1, Correction = &quot;Best&quot;)) ptit tips : la version fonction d’extraction d’élément d’un objet ($) : getElement(object, var) ex : lapply(x, function(x) x$var) = lapply(x, function(x) getElement(x, &quot;var&quot;)) = lapply(x, getElement, &quot;var&quot;) Appliquer une fonction sur plsrs listes/vecteurs : mapply(function(X,Y) {la fonction(X, Y)}, X=list1, Y=list2, SIMPLIFY = F) SIMPLIFY = F : laisse la structure initiale de la liste map functions family font apriori la même chose que les apply avec plus de possiblités et + rapides car écrites en C (https://r4ds.had.co.nz/iteration.html) Les map ne produisent que des vecteurs, pas de matrice) Quand onotre map ne fctne pas il ne renvoie rien. Pour savoir ce qu’il ne va pas : map(safely(fct)) qui renvoie le résultat et le message d’erreur Appliquer une fonction par indices/catégorie : by(data, INDICES = colindex, fct) 1.16 Nettoyage Enlever un élément de l’environement : remove() Vider tt l’envrmt global : rm(list = ls()) ls() renvoie ts les objets de l’envmt 1.17 Boucle Output : créer un objet vide de la classe, voire de la taille que l’on souhaite pour notre output (+ rapide) chaine de charactère vide : ““ vecteur/liste : vector(“numeric”, length(x)), ou juste vector(), list() Quand on ne connait pas la longeur de l’output a priori, on peut l’aggrandir progressivement dans le corps de la boucle : output &lt;- c(output, action qu’on boucle) ou on stocke les outputs de chaque itération dans un élément d’une liste crée en amont (vector(“list”, length(x))), puis qd la loop est finie on unlist (+efficace). En résumé pour les gros outputs, il vaut mieux créer des outputs plus légers à chaque iteration puis tout coller ensemble après la boucle (-lourd). Sequence : créer un vecteur dans lequel puiser s’il n’existe déjà pour agir sur chaque col d’un df : seq_along(df) boucler sur indice, value ou names Body : Pour chaque valeur que peut prendre trait dans All_traits : for(trait in All_traits){} resultat &lt;- rep(NA, 3) for(i in seq_len(3)) { resultat[i] &lt;- i } utiliser [[]] plutot que [] même pour les vecteurs seq_along() : permet dans le cas où la sequence est de taille 0 ou 1 de bien avoir un output de cette taille sinon avec 1:length(1 ou 0) on optiendra 1 0 ou 1 1 faire une boucle dont les résultats sont mis dans une liste: foreach() Boucle while : boucle jusq’à ce que la condition soit fausse. Peut être utilisée lorsqu’on ne connait pas la longueur de la séquence. while (condition) { ## body } Arrêter la boucle : break() Arreter l’itération actuelle et passer à la prochaine : next() répéter jusqu’à ce qu’il n’y est plus d’erreur : repeat { # code if (!(inherits(test,&quot;try-error&quot;))) break } 1.18 Les objets 1.18.0.1 des infos sur mon objet Présence d’une variable “varName” %in% names(df) Type : class(a), pour ttes les variables d’une table lapply(data, class) Structure : str(a, 1) 1 c’est le niveau de structure, yen a plsrs, c’est pas obligatoire 1.18.0.2 List “a” la liste. + Voir les éléments de la liste : a$ + Indicer une liste : a[[position]] Transposer une liste (inverser la tructure) : purr::transpose() 1.18.0.3 Fonction function(arguments){ operation1 operation2 return() ## préciser ce que la fonction doit renvoyer } 1.19 Condition Les if sont des booléens qui vérifient si la “condition” est “TRUE”, donc il n’est pas nécessaire “x == TRUE” mais seulement “x” ou “!x” + if(condition) {action} + if(condition1 &amp;&amp; condition2) { expression } + if(condition1 || condition2) { expression } Dans les if else la place des brackets est importante: if(case1) { expression1 } else { expression2 } if(case1) { expression1 } else if(case2) { expression2 } ... else last.expression ifelse(condition, expression1, expression2) vectorized version of the if() statement. Warning: ifelse() is designed to work with vectors and matrices – not data frames. 1.20 Logical Operators c(T, F, T) | c(F, F, T) ## TRUE FALSE TRUE c(T, F, T) || c(F, F, T) ## TRUE c(F, F, T) || c(F, F, T) ## FALSE c(F, F, T) || c(T, F, T) ## TRUE ## |/&amp; to compare each value of different bolean vectors ## ||/&amp;&amp; to compare single value conditions 1.21 Stan - Modelo - Bayes Chercher une fct dans Rstan : lookup(“bernoulli”) 1.22 Faire de la biblio comme une déesse package bibliometrix 1.23 Date &amp; time Wrong system time : Sys.time() Sys.timezone() To find our time zone : OlsonNames() Set the new time zone : Sys.setenv(TZ = “America/Cayenne”) To change permanently : change time zone of your computer Convert Julian Day to calendar date as.Date(Julian Day, origin = “2021-12-31”) to start the 2022-01-01 convertir une date (y, m, d) en année numeric : as.numeric(format(ExactDate, “%Y”)) convertir une chaine de charactère en date : as_date(col_Date, format = “%m/%d/%Y”) convertir une chaine de charactère en heures : hms::as_hms(col_Hour) ou chron(times = col_Hour) convertir une chaine de charactère en date-time : lubridate::mdy_hms(Time, tz = “America/Cayenne”) mdy pour month, day, year, hms pour hour, minute, seconde. Modifier l’ordre des lettres selon votre format. base::strptime(Time, format = “%Y.%m.%d %H:%M”, tz = “America/Cayenne”) lubridate::as_datetime(, tz = “America/Cayenne”) Convertir des heures en numériques (ex:2.5h) en hh:mm:ss : hms::hms(seconds_to_period(2.5 * 3600)) extraire d’un date_time : date(), year(), month(), day(), hour(), minute(), second(), format(Date_Time, format = “%H:%M:%S”) filtrer selon un pas de temps : Df1 %&gt;% mutate(Time = mdy_hms((Time))) %&gt;% # format: 2023-03-30 18:15:00 filter(minute(Time) %in% c(&quot;00&quot;, &quot;15&quot;, &quot;30&quot;, &quot;45&quot;)) # every 15 minutes filter par date et heures séparément : filter(Date &gt;= as.Date(“2023-03-30”) &amp; Hour &gt;= chron(times=“18:00:00”)) filter par date et heures au format Date-times : filter(Time &gt;= mdy_hms(“03/30/2023 18:00:00”)) temps par ordre croissant data &lt;- data[order(data$Time, decreasing = FALSE),] calcul : Time = Time_UTC-(3 * 60 * 60) pour enlever 3h "],["paralléliser.html", "Cours 2 Paralléliser", " Cours 2 Paralléliser library(foreach) cores = 2 # nbr of cores to use # parallel::detectCores() # 8 i &lt;- NULL j = length(object) # L&#39;enregistrement des clusters cl &lt;- parallel::makeCluster(cores) doSNOW::registerDoSNOW(cl) # Progress bar: pb &lt;- txtProgressBar(min = 0, max = j, style = 3) progress &lt;- function(n) setTxtProgressBar(pb, n) opts &lt;- list(progress = progress) output &lt;- foreach::foreach( i=1:j, .packages = c(&quot;magrittr&quot;), # necessary packages .options.snow = opts # ProgressBar ) %dopar% { print(i) # to check # the function to parallelise: las_to_dem(lases[i], las_path = las_path, mnt_path = mnt_path) } # close progressbar and cluster close(pb) stopCluster(cl) "],["gérer-la-ram---tips.html", "Cours 3 Gérer la RAM - tips !", " Cours 3 Gérer la RAM - tips ! ouvrir le moniteur de performance à côté, si la RAM augmente fortement arréter le calcul développer son code sur un subset des données éviter les répétitions du même calcul les listes prennent moins de place que les vecteurs [[]] permettent de remplacer les objets plutot que de les copier -&gt; plus de place vider l’environnement autant que possible : rm() et gc() parlléliser à ce que peut surporter la RAM de l’ordinateur (regarder la taille des fichiers) memoise:memoise(fct) lancer l’ensemble des calculs sur cluster sur cluster, découper en jobs plutot que paralléliser "],["datatable-functions.html", "Cours 4 datatable functions 4.1 Sources 4.2 Pourquoi data.table 4.3 Syntax 4.4 Create a data.table 4.5 Convert a data.frame or list to a data.table 4.6 Punctuation 4.7 Subset rows 4.8 Extract columns 4.9 Summarize 4.10 Compute columns 4.11 Change column class 4.12 Group 4.13 Common grouped operations 4.14 A sequence operation on a datatable 4.15 Functions ! 4.16 Combine data.tables 4.17 Indexation 4.18 Reshape 4.19 Avoir des infos/calculer des statistiques 4.20 Apply functions 4.21 Sequential rows 4.22 Maniement de chaines de caractères 4.23 Read and write files 4.24 Programmer des fonctions avec data.table", " Cours 4 datatable functions 4.1 Sources https://www.rdocumentation.org/packages/data.table/versions/1.14.2 (R doc) https://larmarange.github.io/analyse-R/manipulations-avancees-avec-data-table.html (French, trés bien!) https://linogaliana.netlify.app/post/datatable/datatable-intro/ (French, trop bien !!) https://www.listendata.com/2016/10/r-data-table.html (efficace) https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html (vignette) https://stackoverflow.com/questions/tagged/data.table (stackoverflow) https://cran.r-project.org/web/packages/data.table/vignettes/datatable-faq.html (frequent questions in dt) 4.2 Pourquoi data.table https://dreamrs.github.io/talks/20180528_RAddicts_datatable.pdf data.table est particulièrement adapté aux données volumineuses (plus de 1Go) où l’utilisation de dplyr est vivement déconseillée data.table est beaucoup plus rapide et puissant que dplyr ne dépend que de R-base, importe uniquement le package “methods” 4.3 Syntax dt[i, j, by] dt : data.table i : rows j : columns/expressions by : grouped columns ces 3 éléments ne sont pas toujours présents Les crochets permettent de ne pas utiliser le “$”. data.table also = to data.frame 4.4 Create a data.table library(data.table) dt &lt;- data.table(a = c(2, 1), b = c(&quot;a&quot;, &quot;b&quot;)) dt[, `:=`(c = 1 , d = 2, e = c(1, 2))] dt &lt;- dt[, -c(2)] ## remove b column (il faut assigner sinon il ne l&#39;enregistre pas) dt[, e := NULL] ## là non dt[, c, by = .(a)] # group dt[, c, keyby = .(a)] ## group and order 4.5 Convert a data.frame or list to a data.table setDT(df) (preferred) or as.data.table(df) setDF(dt) to return in data.frame 4.6 Punctuation les fonctions commençant par set ou contenant l’opérateur := n’ont pas besoin d’être réassigné à l’objet avec “&lt;-”. C’est mieux car ne garde pas de copie en mémoire. le . est un raccourci de list entre [ ] mais pas en dehors := ou :=() (“assignation par référence”) permet de modifier une variable en assignation directe, donc pas besoin de “&lt;-” -&gt; + rapide et très économe en mémoire vive 4.7 Subset rows by rows positions : dt[1:2, ] by values in one or more columns : dt[a &gt; 5, ] both : dt[1:2][a &gt; 5] (ne marche pas pour calculer une colonne, utiliser ifelse) values between two values : dt[a %between% c(2,6)] patern match : dt[a %like% “dep”] ! ne renvoie pas les NA contrairement au data.frame ! La virgule n’est pas obligatoire si pas de colonnes à spécifier. 4.8 Extract columns Les colonnes sont des listes. by position : dt[, c(2)], prefix “-” to drop by names : dt[, .(b, c)] le point est un raccourci de list dans les [] pas en dehors. return as vector : dt[, b] ou dt[[b]] return as data.table : dt[, list(b)] On ne peut pas directement selctionner un var par sa position ou avec une chaine de charactères car compris comme des cstes : dt[, c(“a”), with = FALSE] dt[, !c(“a”, “b”), with = FALSE] remove “a” and “b” columns dt[, c(2:4), with = FALSE] To find patern (like grepl()) %like% ex : dt[,names(dt) %like% “dep”, with=FALSE] patern : “dep” %between% 4.9 Summarize dt[, .(x = sum(a))] create a data.table with new columns based on the summarized values of rows. Summary functions ex : mean(), median(), min(), max(), etc dt[, lapply(.SD, mean), .SDcols = c(“a”, “b”)] for several columns dt[, lapply(.SD, mean)] for all the columns dt[, sapply(.SD, function(x) c(mean=mean(x), median=median(x)))] multiple statistics .N pour obtenir le nombres d’observations ,by = par sous-groupe 4.10 Compute columns := permet de modifier une variable en assignation directe, donc pas besoin de “&lt;-”. (“c”) pour dire que ce sont des colonnes existantes et non des nouvelles a creer dt[, c := 1 + 2] : compute a column based on an expression (c = col name) dt[a == 1, c := 1 + 2] : compute a column based on an expression but only for a subset of rows (condition) Si on veut mettre des NA ! : NA_integer_, NA_real_, NA_character_ (mettre juste NA pose des problèmes de classe) dt[, :=(c = 1 , d = 2)] : compute multiple columns based on separate expressions (se créent en meme temps donc ne peut pas dépendre de l’autre) ou dt[ , c(“newcol1”, “newcol2”) := list(col1 * 10, col2 * 20)] With an ifelse : dt[, c := ifelse(min &lt; 50, 1,0)] (1 if TRUE, 0 if FALSE) Delete a column : dt[, c := NULL] (trés rapide) Séparer 1 col en 2 : dt[, c(“cola”, “colb”) := tstrsplit(col, “_“, fixed = TRUE)] Unire 2 col en 1 : dt[, xy:= paste0(x,y)] ou dt[, xy:= paste(x,y, sep = “_“)] 4.11 Change column class dt[, b := as.integer(b)] 4.12 Group dt[, j, by = “colname”] : by 1 var dt[, j, by = .(a)] : group rows by values in specified columns (a the column). dt[, j, by = c(“var1”, “var2”, “var3”)] dt[, j, keyby = .(a)] : group and simultaneously order rows by values in specified columns. 4.13 Common grouped operations dt[, .(c = sum(b)), by = a] : summarize rows within groups. dt[, c := sum(b), by = a] : create a new column and compute rows within groups. dt[, .SD[1], by = a] : extract first row of groups. dt[, .SD[.N], by = a] : extract last row of groups. dt[, .(new = old, new = col1 + col2)] : rename a col, and compute another 4.14 A sequence operation on a datatable Pour enchaîner les opérations : dt[…][…] Règle : pas faire de lignes trop longues et crochet de fin d’op et celui de début de la prochaine doivent être accolé : ][ 4.15 Functions ! On peut y appliquer n’importe quelle fct de n’importe quel pkg, mais préférer celles du pkg data.table. Remove rows with missing values : na.omit(dt, cols=c(cols1, col2), invert=FALSE) cols et invert ne sont pas obligatoires Order columns values : setorder(dt, a, -b) “-” to decreasing order ou dt[order(a, -b)] Extract unique rows on specified col unique(dt, by = c(“a”, “b”)) no by to use all columns Count the nbr of unique rows : uniqueN(dt, by = c(“a”, “b”)) Rename col : setnames(dt, c(“old1”, “old2”), c(“new1”, “new2”)) ou dt[, .(new = old)] changer ordre des colonnes : setcolorder(DT, c(“col1”,“col2”)) avec “:=” : dt[ , col_min := tolower(col)] Moyenne entre 2 colonnes par ligne : data[, Var := rowMeans(.SD), .SDcols = c(“Var1”, “var2”)] 4.16 Combine data.tables Avec une fct : data.table::merge(x, y, all.x = TRUE, [options]) (comme les join tidy, voir les options) all.x = TRUE permet de garder les colonnes de x même si elles ne matchent pas avec y **merge(dataA, dataB, by=“A”,all=TRUE, suffixes = c(“_A”, “_B”))** Join on rows with equal values : dt_a[dt_b, on = .(b = y)] ‘on’ pour spécifier les col équivalentes mais de noms différents on equal and unequal : dt_a[dt_b, on = .(b = y, c &gt; z)] Rolling join Combine rows of 2 datatables : rbind(dt1, dt2) or rbindlist(list(dt1, dt2), use.names=TRUE, fill=TRUE) (+ rapide) Combine columns of 2 datatables : cbind(dt1, dt2) 4.17 Indexation Très puissant pour accélérer les opérations sur les lignes (filtres, jointures) Déclarer les variables faisant office de clef (key) : setkey(dt, a, b) ou setkeyv(dt, c(“a”,“b”)) ou setkey(dt, a) si qu’1 var. -&gt; réordone selon cette clef Peut prendre bcp de temps (qqmin)mais gros gain de temps pour les opérations ultérieures. Pour savoir si un data.table est déjà indexé : key(dt) qui renvoie le nom des clés ou NULL. Supprimer la clef : 4.18 Reshape https://rdatatable.gitlab.io/data.table/articles/datatable-reshape.html Long to wide format : dcast(dt, id ~ y, value.var = c(“a”, “b”), fun.aggregate = list(mean, sum)) with: y = col name in the longformat whose values will become the columns of the wide format value.var : col name whose values in columns will be put in rows (colonnes à transposer) fun.aggregate : si on veut utiliser des fcts pour aggrége les données Pour des noms plus significatifs des nouvelles colonnes : id ~ paste0(“y”, y) (préciser l’appartenance à y à chaque nom de col) Conseil : Il faut donc faire attention à ce que ces variables aient un nombre limité de valeurs, pour ne pas obtenir une table extrêmement large. On peut éventuellement discrétiser les variables continues, ou regrouper les modalités avant d’utiliser dcast() Wide to long format (ce qu’on veut!) : melt(dt, id.vars = c(&quot;id&quot;), measure.vars = patterns(&quot;^a&quot;, &quot;^b&quot;), variable.name = &quot;y&quot;, value.name = c(&quot;a&quot;, &quot;b&quot;)) with : id.vars : variables qui identifient les lignes de table d’arrivée; col qui restent des cols. measure.vars : cols to rows (variables qui sont transposées) variable.name : new col name for values in rows to this column (nom de la nouvelle colonne qui contient le nom des variables transposées) value.name : new col names (nom de la nouvelle colonne qui contient la valeur des variables transposées.) 4.19 Avoir des infos/calculer des statistiques dt[, sum(b)] : pour renvoyer la valeur Pour en calculer plsrs à la fois : dt[, .(mean = mean(b), sd = sd(b))] .N : nbr d’obs uniqueN : nbr d’obs uniques %in% : nbr dans la liste %chin% : character dans la liste %between% : valeur entre deux nombres %like% “^x” : Reconnaissance d’une chaîne de caractères (comme grepl()) rowid(var1, var2) : créer un identifiant de ligne unique pour un goupe de variables (= un compteur) 4.20 Apply functions dt[, lapply(.SD, mean), .SDcols = c(“a”, “b”)] (create a new table) dt[ , names(dt) := lapply(.SD, as.character)] for all the columns dt[, (cols) := lapply(.SD, as.character), .SDcols = cols] for specified columns (cols = c(“a”, “b”)) () pour dire que ce sont des colonnes existantes et non des nouvelles a creer dt[, paste0(cols, “_m”) := lapply(.SD, mean), .SDcols = cols] add a suffix .SD : “Subset of Data” (= mot clef de data.table, représentant les colonnes) .SDcols : columns on which to apply 4.21 Sequential rows Sub Queries (like SQL) : DT[ ] [ ] [ ] 4.22 Maniement de chaines de caractères séparer une colonne en fonction d’un caractère : data.table::tstrsplit() 4.23 Read and write files Lecture et ecriture de fichiers plats (= fichiers texte) Import : fread(“file.csv”, select = c(“a”, “b”)) + Très rapide pour importer des gros volumes de données et nettement plus rapide que les fcts du pkg readr. + Permet de selectionner les colonnes qu’on veut ou ne veut pas importer + un grand nbr d’options Export : fwrite(dt, “file.csv”) 4.24 Programmer des fonctions avec data.table 4.24.1 Sécuriser Quand on utilise “:=” on peut par erreur écraser des données. Donc la sécurité c’est de faire une copie des input en entrée de fonction et travailler sur cette copie (qui porte un autre nom). 4.24.2 Variables en argument Pas necessaires si cest de la valeur en chaine de characteres dont on a besoin. Si on a besoin du symbol/name, ils nous faut ces outils : https://linogaliana.netlify.app/post/datatable/datatable-nse/ get() ne fctne que si l’argument n’a pas le même nom que la colonne du dt eval() ne fctne que si l’argument a le même nom que la colonne du dt modif_variable &lt;- function(dt, variable = &quot;x&quot;){ dt_copy &lt;- data.table::copy(dt) ## on copie pour ne pas que les prochaines actions se fasse la version originale dt_copy[,c(variable) := get(variable) + 1] ## get() pour appeler un nom de variable entre guillemets (character) return(dt_copy) } Select columns whose name is variable, using a character vector , with = FALSE] IdTree = &quot;idTree&quot; Plot = &quot;plot&quot; SubPlot = &quot;subplot&quot; AssoVect &lt;- c(IdTree, Plot, SubPlot, TreeFieldNum) correspondances &lt;- unique(Data[, ..AssoVect]) ## &quot;..&quot; correspondances &lt;- unique(Data[, c(IdTree, Plot, SubPlot, TreeFieldNum), with = FALSE]) ## with = FALSE : the column names can be used as variables Cas plus générique : eval &amp; substitute eval() exécute une expression substitute() : attribut les valeurs, substitut les variables (=names=symbol) par leurs valeurs as.name() : refer to R object by their name -&gt; avoid conflict and work with user variables names df1 &lt;- data.table(sp = LETTERS[1:10], x = rnorm(10)) ## cas où l&#39;arg n&#39;a pas le même nom que la col df2 &lt;- data.table(species = LETTERS[1:10], var = rnorm(10)) ## cas où l&#39;arg a le même nom que la col species = &quot;sp&quot; var = &quot;x&quot; fun &lt;- function(df, species, var){ env &lt;- lapply(list(.species = species, .var = var), as.name) ## environment eval(substitute( { df[, p := paste(.species, .var)] df[, q := paste(.species, &quot;_&quot;, .var)] ## le . devant le nom n&#39;est pas nécessaire c&#39;est juste mieux de distinguer pour le codeur }, env)) return(df) } fun(df1, &quot;sp&quot;, &quot;x&quot;) ## cas où l&#39;arg n&#39;a pas le même nom que la col df1 fun(df2, &quot;species&quot;, &quot;var&quot;) ## cas où l&#39;arg a le même nom que la col df2 4.24.3 .SD (Subset of Data) .SD permet d’appliquer la même opération sur plusieurs colonnes. .SDcols : sélection des colonnes sur lesquelles appliquer l’opération (par défaut elles sont toutes prises) -&gt; syntaxe très puissante, compact et lisible 4.24.4 lapply+.SD pour des fonctions de statistiques descriptives ## fabriquer une table statistique DT[, lapply(.SD, min), .SDcol = &quot;a&quot;, by = c] mes_statistiques &lt;- function(x) return(c(mean(x), var(x), quantile(x, probs = c(.25,.5,.75)))) data_agregee &lt;- dt[, ## i lapply(.SD, mes_statistiques), ## j (expression) by = &quot;Species&quot;, ## by (group) .SDcols = c(&quot;Petal.Width&quot;,&quot;Petal.Length&quot;)] data_agregee[, &#39;stat&#39; := c(&quot;moyenne&quot;,&quot;variance&quot;,&quot;P25&quot;,&quot;P50&quot;,&quot;P75&quot;), by = &quot;Species&quot;] ## add a col to define the values in rows data_agregee 4.24.5 Des data.table poupées russes Facilite la parallélisation DT[, list(list(.SD)), by = Group] cl &lt;- makeCluster(2) DT[, clust := list(parLapplyLB(cl, V1, function(X){kmeans(X,2)$cluster}))] "],["spatial-data-cours.html", "Cours 5 Spatial data cours 5.1 Tips 5.2 Mapper des données 5.3 Functions", " Cours 5 Spatial data cours Contient pas mal d’infos : https://r-spatial.org/book/07-Introsf.html Géomatique avec R : https://rcarto.github.io/geomatique_avec_r/ library(tidyverse) library(sf) 5.1 Tips Le CRS est souvent une source de bug : absent, pas le bon, contenant des accents 5.2 Mapper des données Mapper des points dont on a la longitude et la latitude : ggplot(mapping = aes(x = lon, y = lat)) + geom_point() ggmap : ajoute des couches d’images (fond de carte) téléchargées depuis des services de cartorgaphie en ligne. library(&quot;ggmap&quot;) east_canada &lt;- get_stamenmap(bbox = c(left=-81, right = -59, bottom = 44, top = 51),#boîte de coordonnées délimitant la carte à produire zoom = 6, #zoom = niv de détails (2 suffisant pour une carte du monde) maptype = &quot;terrain&quot;) # type de carte ggmap(east_canada) + geom_point(data = weather, mapping = aes(x = lon, y = lat)) ## ou ggmap(east_canada, base_layer = ggplot(weather, aes(x = lon, y = lat))) + geom_point() ##base_layer permet d’effectuer des facettes et d’éviter de spécifier la source des données dans toutes les couches subséquentes. Maptype : - “terrain” : efficace mais peu esthétique - “toner-lite” : pour l’impression - “watercolor” : pour le web 5.2.1 Types de données spatiales Les données spatiales sont des données localisées dans un système de coordonnées de référence (CRS) Données vectorielles (en général en 2D, 3D si on prend en compte l’altitude: Données ponctuelles : points Données linéaires : série de points (route, rivière) Données de polygone : aire délimitée par des points (champ, bassin versant) Données raster: grille (image satellite où chaque pixel est associé à un recouvrement foliaire.) Données génériques : shapefiles et geojson Données spécialement conçus pour R : sf 5.2.2 ggplot geom_polygon : : pour créer des polygones coord_map geom_path() : pour créer des lignes geom_tile() : visualiser une grille -&gt; graphique de type “heatmap” *geom_sf() : pour afficher les objets sf Cartes intéractives (mode leaflet) : tmap_mode(“view”) Pour revenir en mode statique : tmap_mode(“plot”) 5.2.3 ggspatial ajouter une échelle : annotation_scale(location = “br”) + ajouter une boussole : annotation_north_arrow(pad_y = unit(1, “cm”),style = north_arrow_nautical()) 5.2.4 Les rasters = données (variable(s)) associées à une grille comprenant les combinaisons de longitudes et latitudes. Raster = * une en-tête : - syst de coordonnées de référence - l’origine : généralement les coordonnées du coin bas-gauche de la matrice (mais le package ‘raster’ prends le coin haut-gauche par défaut) - dimensions : nbr de colonnes, de lignes, la résolution de la taille des cellules * une matrice de cellules équidistantes (~pixels) : la matrice ne stocke qu’1 coordonnée de la cellule : l’origine -&gt; + efficace et rapide que le traitement des données vectorielles. 1 cellule = 1 ou plsrs valeurs, ou NA (numérique ou catégorique) = la valeur moyenne (ou majoritaire) de la zone qu’elle couvre. Cependant, dans certains cas, les valeurs sont en fait des estimations pour le centre de la cellule. l’information spatiale est implicitement donnée par l’étendue spatiale et le nombre de lignes et de colonnes dans lesquelles la zone est divisée. Les données raster sont en général continues (altitude, T°, densité de pop), mais peuvent-être aussi catégoriques (type de sol) mais dans ce cas une représentation vectorielle pourrait être plus appropriée. expand.grid() : créer une grille de paramêtres : associer des coordonées à une variable grid &lt;- expand.grid(lon = seq(from = -80, to = -60, by = 0.25), lat = seq(from = 45, to = 50, by = 0.25)) grid &lt;- grid %&gt;% mutate(z = 10*sin(lon*lat) - 0.01*lon^2 + 0.05*lat^2) # créer une variable spatialisée grid %&gt;% head() ggplot(grid, aes(lon, lat)) + geom_tile(aes(fill = z)) Géostatistique = étude statistique des variables spatiales 5.2.4.1 Geometry Intersection between a raster and a geometry (sf) : mask(raster, geometry) 5.2.5 Les objets spatialisés en R 5.2.5.1 Données vectorielles Objets géoréférencés R (=variables (data.frame) liés à des coordonées (sfc)): sf Packages : sp, sf (mieux adapté au tidy) Transformer un dataframe en objet sf : st_as_sf(, coords = c(‘X’,‘Y’,‘Z’)) : - type de géométrie (geometry type: POINT (= sfg objets)), - les limites des objets (bbox: …), - le système de référence (epsg ou proj4string: …) - le tableau descriptif Revenir à une table non spatialisée : st_drop_geometry() Les objets sfg peuvent être créés avec : * un vecteurs numérique * une matrice * une liste plot(st_point(c(5, 2))) # XY point (2D) st_point(c(5, 2, 3)) # XYZ point (3D) st_point(c(5, 2, 1), dim = &quot;XYM&quot;) # XYM point st_point(c(5, 2, 3, 1)) # XYZM point ## the &#39;rbind&#39; function simplifies the creation of matrices ### MULTIPOINT multipoint_matrix = rbind(c(5, 2), c(1, 3), c(3, 4), c(3, 2)) plot(st_multipoint(multipoint_matrix)) ### LINESTRING linestring_matrix = rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)) plot(st_linestring(linestring_matrix)) ### POLYGON polygon_list = list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))) plot(st_polygon(polygon_list)) ### POLYGON with a hole polygon_border = rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)) polygon_hole = rbind(c(2, 4), c(3, 4), c(3, 3), c(2, 3), c(2, 4)) polygon_with_hole_list = list(polygon_border, polygon_hole) plot(st_polygon(polygon_with_hole_list)) ### MULTILINESTRING multilinestring_list = list(rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)), rbind(c(1, 2), c(2, 4))) plot(st_multilinestring((multilinestring_list))) ### MULTIPOLYGON multipolygon_list = list(list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))), list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2)))) plot(st_multipolygon(multipolygon_list)) ### GEOMETRYCOLLECTION gemetrycollection_list = list(st_multipoint(multipoint_matrix), st_linestring(linestring_matrix)) plot(st_geometrycollection(gemetrycollection_list)) ##&gt; GEOMETRYCOLLECTION (MULTIPOINT (5 2, 1 3, 3 4, 3 2), ##&gt; LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2)) 5.2.5.1.1 Simple feature columns (sfc) une sfc est une liste d’objets sfg, de meme type géométrique ou non. point1 = st_point(c(5, 2)) point2 = st_point(c(1, 3)) points_sfc = st_sfc(point1, point2) #st_sfc() crée une liste d&#39;objets sfg points_sfc st_geometry_type(points_sfc) sfc to sfg : st_point(as.numeric(unlist(sfc))) (ex ici point) st_read() : pour charger des données (ex:shapefiles) au format sf Les tableaux sf sont manipulables sous tydiverse. plot() : par défaut crée un multi-panel plot : un graph par variable. Pour ne ploter que les contours (la géométrie): quebec %&gt;% st_geometry() %&gt;% plot() # ou bien plot(st_geometry(quebec)), ou bien plot(quebec %&gt;% select(geometry)) Jointures spatiales (intersections de polygones, unions, différences) -&gt; 1 nouvelle géométrie Attribuer un crs à un objt sf ou sfc : st_set_crs(objtsanscrs, st_crs(objtaveccrs)) st_crs&lt;- : replacing crs does not reproject data; use st_transform for that Exporter un tableau sf en format csv incluant la géométrie : st_write(obj = tableau,dsn = “tableau.csv”, layer_options = “GEOMETRY=AS_XY”) Si la géométrie n’est pas consituée de points, il faudra préalablement transformer les polygones en points avec st_cast() 5.2.5.2 Données raster Package : raster grilles svt enchasées dans des images tif géoréférencées. 1 image tif = 1 ou plsrs bandes (variables) raster() : importe des données raster à 1 bande (“names” dans les infos) brick() : importe des données raster à plsrs bandes (“names” dans les infos) stack() : permet de connecter plusieurs objets raster stockés dans différents fichiers ou plusieurs objets en mémoire Les informations des objets RasterLayer et RasterBrick peuvent être extraites par les fonctions : extent(), ncell(), nlayers() et crs(). La fonction plot() permet d’explorer les données en créant 1 graphique par bande. Les tif sont trés volumineux. Si l’on n’as pas besoin d’une si haute résolution, on peut simplifier le raster avec raster::aggregate() Attribuer un crs à raster : crs(mnt) &lt;- crs(obj2) ou crs(mnt) &lt;- crs(“EPSG:2972”) 5.2.5.3 Travailler sur du vectoriel &amp; du raster Le package “raster” ne supporte pas le format sf, il faut donc préalablement convertir en sp : poly_sp &lt;- as(poly, “Spatial”) (poly étant ici un polygone) 5.2.5.4 Opérations sur Raster Le package terra : https://rcarto.github.io/geomatique_avec_r/les-donn%C3%A9es-raster-le-package-terra.html# Intersection (ce qu’il y a en commun) entre un polygone et un raster : mask() Découper (rectangulaire selon les limites de l’objet) : crop() Découper un raster selon un autre raster, ou mettre à la meme résolution :resample() Extraction : extract() ##Pour effectuer un calcul sur l’intérieur du polygone avec extract()… on spécifie le raster, le polygone et la fonction! extract(canopy, poly_sp, fun = mean) # &quot;canopy&quot; = raster, &quot;poly_sp&quot; = polygone Convert values to classes : cut() : # Reclasser en intervalles avec `cut()` MNT &lt;- stars::st_as_stars(MNT) # first transform in stars MNT &lt;- raster::cut(MNT, breaks = c(0, 6, 23.67), labels = c(&quot;0-6&quot;, &quot;6-23.67&quot;), include.lowest = TRUE) library(raster) shpP16 &lt;- rgdal::readORG(dsn = &quot;C:/Users/BADOUARD/Desktop/Vincyane/Gaps2019/Gaps2019&quot;, layer = &quot;Gaps2019&quot;) shpP16 &lt;- raster::shapefile(&quot;C:/Users/BADOUARD/Desktop/Vincyane/Gaps2019/Gaps2019.shp&quot;) library(sf) shpP16 &lt;- st_read(&quot;C:/Users/BADOUARD/Desktop/Vincyane/Gaps2019/Gaps2019.shp&quot;) plot(shpP16) plot(shpP16, col=&quot;#f2f2f2&quot;, bg=&quot;skyblue&quot;, lwd=0.25, border=0 ) library(ggplot2) ggplot() + geom_sf(data = shpP16, size = 3, color = &quot;black&quot;, fill = &quot;red&quot;) + ggtitle(&quot;2019 - P16 gaps&quot;) + coord_sf() 5.2.5.5 Plot raster with ggplot # First transform raster in data.frame TWI_df &lt;- as.data.frame(TWI, xy = TRUE)%&gt;% na.omit() ggplot() + geom_raster(data = TWI_df, aes(x = x, y = y, fill = `P13_2022_TWI`)) + # with geom_raster() scale_fill_viridis_c() + ggtitle(&quot;Paracou 2019 P13 - TWI&quot;) + coord_sf() 5.2.6 Shapefiles lire un shapefile : st_read() Contiennent plsrs fichiers : fichier .prj : informations du système de coordonnées 5.2.7 Systèmes de coordonnées de références (CRS) Le vérifier : crs() L’attribuer : obj &lt;- st_set_crs(obj, 2972) en UTM c’est en mètres 5.2.7.1 Système de coordonnées géographiques Valeurs : Longitude/latitude La Terre est représenté sphérique ou ellipsoïde (+juste). Modele ellipsoïde, 2 paramètres: - rayon équatorial - rayon polaire 5.2.7.2 Système de référence de coordonnées pojetées basé sur des coordonées catésiennes sur une surface plane. et basés sur les systèmes de coordonnées géographiques une origine axes x &amp; y une unité (ex: le mètre) 5.3 Functions chercher là dedans : https://r-spatial.org/book/07-Introsf.html Retourner des points interpolés selon une interpolation linéaire entre 2 points : approx(x= coord, y = autrecoord, xout = coord selon laquelle interpoler) 5.3.1 sf package Passer d’un sf à un dataframe (supprimer la col géométrie) : st_geometry(sf) &lt;- NULL # from sf to df with original coordinates XY &lt;- st_coordinates(datasf) # stock coordinates st_geometry(datasf) &lt;- NULL # no more sf object data &lt;- cbind(datasf, XY) # bind XY coord sfg to sfc : st_sfc(sfg) sfc to sfg : st_point(as.numeric(unlist(sfc))) (ex ici point) MULTIPOINT to POINT : st_cast(st_sfc(mutipoint), “POINT”, group_or_split = TRUE) Simplification d’objet : st_cast(objet, “objt + simple” (ex : polygon to linestring) Attribuer un crs à un objt sf ou sfc : st_transform(objtsanscrs, st_crs(objtaveccrs)) Un buffer autour d’un objet spatial: buffer &lt;- st_buffer(pol, dist=100, endCapStyle = “SQUARE”) %&gt;% st_union() # the square doesn’t works corners of a square : st_coordinates(square)[,1:2] st_union ne préserve pas l’ordre des géométries dans la géométrie finale, il faut utiliser : objt_sf_12 &lt;- objt_sf_1 %&gt;% rbind(objt_sf_2) objt_sf_12 &lt;- do.call(c, st_geometry(objt_sf_12)) Create a regular grid : # Subplots (1ha) grid for 25 ha grid &lt;- data.frame(expand.grid( x = seq(0,500, by= 100), y = seq(0,500, by= 100) ) ) plot(grid) 5.3.2 Stars package to work on rasters voir : https://r-spatial.org/book/07-Introsf.html ### en 3D + package : plot3D for 3-D arrows, segments, polygons, boxes, rectangles 3D fixe plot3D::scatter3D() rgl for visualisation in 3D 3D pivotable : plotly::plot_ly(data, x = ~x, y = ~y, z = ~z, color=var) rgl::plot3d() "],["geometry-1.html", "Cours 6 Geometry", " Cours 6 Geometry Compute solid angle from plane angle : un angle solide (omega) est l’analogue tridimensionnel de l’angle plan ou bidimensionnel. unité : stéradian (sr) theta = 11.8 * pi/180 # en radian omega = 2pi(1-cos(theta/2)) # /2 pour demi angle "],["work-with-lidar-data.html", "Cours 7 Work with LiDAR data 7.1 Tools 7.2 Load laz 7.3 Write laz 7.4 Check if the las is complete 7.5 See their summaries 7.6 Plot 7.7 Trajectory 7.8 Clip the point cloud 7.9 Classification 7.10 Ajouter des points à un las, combiner plusieurs las 7.11 Caluler des métriques", " Cours 7 Work with LiDAR data LiDAR : “light detection and ranging” ou “laser imaging detection and ranging” Lumière d’un laser en général : visible, infrarouge ou ultraviolet. Télémétrie : détermination de la distance d’un objet Principe d’écholocalisation : La distance est donnée par la mesure du délai entre l’émission d’une impulsion et la détection d’une impulsion réfléchie, connaissant la vitesse de la lumière. Mesure de la matière : isoler l’effet des différentes interactions entre la lumière et la matière le long du faisceau laser. On appelle « équation lidar » le bilan de liaison du lidar entre son émission et sa réception, c’est-à-dire l’énergie lumineuse E (en J) de l’impulsion rétrodiffusée par une cible (supposée lambertienne, i.e. qui diffuse la lumière uniformément) située à une distance z et captée par le lidar. Emet de plusieurs dizaines à plusieurs centaines de milliers d’impulsions chaque seconde. Fichiers : - trajectoire : décrit les positions dans le temps - nuage de points (.las ou .laz en compréssé) 7.1 Tools logiciels : cloudcompare, Fugroviewer 7.1.1 LAStools Using LAStools, help here. In R : # Define the path for lastools executables LAStoolsDir = &quot;D:/Program/LAStools/bin/&quot; # Define the R function that calls lastools executables LAStool &lt;- function(tool, inputFile, ...){ cmd = paste(paste(LAStoolsDir, tool, sep=&#39;&#39;), &#39;-i&#39;, inputFile , ...) cat(cmd) system(cmd) return(cmd) } # Define the directory for the ALS project cloudPath &lt;- &quot;//amap-data.cirad.fr/work/users/VincyaneBadouard/Lidar/HovermapUAV2023/P16_C19C20/&quot; # Define las/laz files to be processed inFiles = paste(cloudPath, &#39;Output_laz1_4.laz&#39;, sep =&#39;&#39;) outFile = &quot;FullDensity.laz&quot; # Define output directory (and creates it ... if doesn&#39;t exist) outDir = paste(cloudPath, &#39;FullDens_split&#39;, sep =&#39;&#39;) dir.create(outDir) # , showWarnings = F (pas le droit de le faire dans le serveur safe) # Call the lastools function to split in several files by gpstime cores &lt;- detectCores() LASrun &lt;- LAStool(&#39;lassplit64&#39;, inFiles, &#39;-cores&#39;, cores, &#39;-by_gps_time_interval 60&#39;, &#39;-odir&#39;, outDir, &#39;-odix _split&#39;, &#39;-o&#39;, outFile ) # Merge files once corrected outDir2 &lt;- paste(cloudPath, &#39;FullDens_cor&#39;, sep=&#39;&#39;) # new folder: files rebinded and corrected dir.create(outDir2, showWarnings = F) inFiles &lt;- paste(outDir, &#39;/*.laz&#39;, sep=&#39;&#39;) # files corrected LASrun &lt;- LAStool(&#39;lasmerge64&#39;, inFiles, &#39;-odir&#39;, outDir2, &#39;-odix _cor&#39;, &#39;-o&#39;, outFile ) # laz to dem las2dem -i input/Paracou_284500_581500.laz -o output/dem.asc 7.1.2 R packages lidR : https://github.com/r-lidar/lidR ; https://r-lidar.github.io/lidRbook/ 7.2 Load laz Le format binaire ASPRS LASer (.las) est un format ouvert permettant le stockage de données à 3 dimensions sous forme binaire. En plus des coordonnées x,y,z de chaque point, ce format permet de stocker des informations propres aux données LiDAR : - temps GPS - intensité - angle de scan - numéro du retour - classification - données utilisateurs - nombre de retours par impulsion - intensité du rouge - intensité du vert - intensité du bleu - indicateur de bord de ligne de vol - ID du point source - couleur RGB https://r-lidar.github.io/lidRbook/io.html las1 &lt;- lidR::readLAS(files = &quot;D:/Mes Donnees/PhD/SIG_data/Lidar/P16_2022_ptf6.laz&quot;) # ou las2 &lt;- rlas::read.las(files = &quot;D:/Mes Donnees/PhD/SIG_data/Lidar/P16_2022_ptf6.laz&quot;) 7.2.1 Load several laz # readLAScatalog() creates a las catalog from a folder to search in ctg &lt;- readLAScatalog(folder = &quot;D:/Mes Donnees/PhD/Lidar&quot;, filter = &quot;keep&quot;) # las catalog. filter only 1st returns # readLAS(filter = &quot;-help&quot;) # &quot;-first_only&quot; # , select = &quot;xyz&quot;) # load XYZ only # filter = &quot;-keep_first -drop_z_below 5 -drop_z_above 50&quot;) # To apply a function to a LAScatalog catalog_apply(ctg,fct) 7.2.2 Load/Create a subset of the laz to developpe the code with smaller data range(las$gpstime) # 1697733332 1697734174 readLAS(filter = &quot;-help&quot;) las &lt;- readLAS(&quot;Y:/users/VincyaneBadouard/Lidar/HovermapUAV2023/Translation/LAZ/UAV_C19C20_translated.las&quot;, filter = &quot;-keep_gps_time 1697733332 1697733334&quot;) sampletowork &lt;- decimate_points(las, random(1)) # reduce point density (13 MB) plot(sampletowork) 7.3 Write laz writeLAS(las, &quot;Z:/users/VincyaneBadouard/ALS2022/P16_2022_v2_4ha.laz&quot;) 7.4 Check if the las is complete lidR::las_check(las1) 7.5 See their summaries class(las1) # [1] &quot;LAS&quot; # attr(,&quot;package&quot;) # [1] &quot;lidR&quot; class(las2) # &quot;data.table&quot; &quot;data.frame&quot; print(las1) print(las2) # area : 294704 m² # points : 116.93 million points # density : 396.77 points/m² # density : 180.46 pulses/m² 7.6 Plot lidR::plot(las1) # persp(x = las2$X, y = las2$Y, z = las2$Z) LASfile &lt;- system.file(&quot;extdata&quot;, &quot;MixedConifer.laz&quot;, package=&quot;lidR&quot;) las &lt;- readLAS(LASfile) lidR::plot(las) Ploter une raster layer : d’abord en faire un data.frame TWI_df &lt;- as.data.frame(TWI, xy = TRUE) %&gt;% na.omit() %&gt;% dplyr::rename(&#39;TWI&#39; = &#39;P13_2022_TWI&#39;) ggplot() + geom_raster(data = TWI_df, aes(x = x, y = y, fill = TWI)) + scale_fill_viridis_c() + ggtitle(&quot;Paracou P13 - TWI&quot;) + coord_sf() 7.7 Trajectory Permet : - de calculer les hauteurs de vol - d’estimer les incertitudes de positionnement des points (en fonction de la hauteur) - de normaliser les intensités des échos (si cela n’a pas été fait par le fournisseur) Contient : - The X axis is tangent to the geoid and is pointing to the north, the Z axis is pointing to the center of the earth, and the Y axis is tangent to the geoid and is pointing to the East. - temps GPS (seconde) - Latitude et longitude (radian) - Altitude (mètre) - Vitesse dans l’axe X (est-ouest), Y (nord-sud), Z (verticale) (m/s) - Quaternions q0, q1, q2, q3 - “r”, “g” ,“b” : red, green, blue - “nx” , “ny” , “nz” : coordonnées du normal vector - “roll” (roulis) : roll angle φ which is defined around the xb axis (radian) - “pitch” (tangage) : the pitch angle θ which is defined about the intermediate axis y1 (radian) - “yaw” (lacet) : the yaw angle ψ which is defined around the vertical axis zi (radian) - Angle du système d’axes par rapport au Nord (radian) - Accélération sur les axes X, Y, Z (m/s2) - Vitesse angulaire autour des axes X, Y, Z (rad/s) traj &lt;- fread(&quot;Z:/users/VincyaneBadouard/ZebHorizon/LiDAR_Manu/2022-09-07_15-46-34.gs-traj&quot;) # or .txt traj plot3d(traj[,2:4], aspect=F) # plot the trajectory (x,y,z) 7.8 Clip the point cloud selon une géométrie : clip_roi(las = ctg, geometry = ROI) selon des coordonées : clip_rectangle()/clip_polygon()/clip_circle() sur un LAScatalog (ça ne coupera pas dans une dalle mais ça réduit le nombre de dalle à la région d’intéret) : catalog_intersect(ctg, ROI) 7.9 Classification 7 = bruit 2 = sol 9 = eau vegetation = 0,3,4,5 table(ST@data$Classification) # 7 = bruit, 2 = sol, veg = 3,4,5 ST@data &lt;- ST@data[Classification != 7,] # enlever le bruit Classifier le sol # Modèle numérique de terrain mnt &lt;- rast(&quot;Z:/users/VincyaneBadouard/ALS2022/mnt_roi36ha_1m.asc&quot;) plot(mnt) # Normaliser la hauteur du sol ST_norm &lt;- lidR::normalize_height(ST, mnt) # applati le relief plot(ST_norm)# sol plat # Classifier les points ST_norm@data[(Z&gt;(-0.2) &amp; Z&lt; 0.5), Classification := 2] table(ST_norm@data$Classification) # 7 = bruit, 2 = sol, vegetation = 3,4,5 # plot(ST_norm, color = &quot;Classification&quot;) # trop long # Dénormaliser la hauteur du sol ST &lt;- lidR::unnormalize_height(ST_norm) 7.10 Ajouter des points à un las, combiner plusieurs las # transform in a LAS object las2 &lt;- LAS(vector_coord[,.(XB,YB,ZB)], las1@header) # This triggers some warning because we input some incorrectly quantized coordinates. Check the output of las_check() las_check(las2) # We can fix that las2 &lt;- las_quantize(las2, TRUE) las2 &lt;- las_update(las2) las_check(las2) # To combine the las1 and las2 they must have the same columns. # We need to make them manually. # Here if you load only XYZ in las the job is easier. las2@data$gpstime = 0 las2@data$Intensity = 0L las2@data$ReturnNumber = 1L las2@data$NumberOfReturns = 1L las2@data$ScanDirectionFlag = 0L las2@data$EdgeOfFlightline = 0L las2@data$Classification = 0L las2@data$Synthetic_flag = FALSE las2@data$Keypoint_flag = FALSE las2@data$Withheld_flag = FALSE las2@data$ScanAngleRank = 0L las2@data$UserData = 0L las2@data$PointSourceID = 0L # Bind las3 &lt;- lidR::rbind(las1, las2) 7.11 Caluler des métriques 7.11.1 Generate a CHM (Canopy Height Model) and DSM (Digital Surface Model) In the case of a normalized point cloud, the derived surface represents the canopy height (for vegetated areas) and is referred to as CHM. When the original (non-normalized) point cloud with absolute elevations is used, the derived layer represents the elevation of the top of the canopy above sea level, and is referred to as DSM CHM &lt;- rasterize_canopy(normalize_height(ST, knnidw()), res = 1, algorithm = p2r()) # p2r() : for each pixel of the output raster the function attributes the height of the highest point found # tin() : triangulation algorithm plot(CHM, col = height.colors(25)) DSM &lt;- rasterize_canopy(ST, res = 0.5, algorithm = dsmtin()) # dsmtin() : algorithm for digital surface model computation using a Delaunay triangulation of first returns with a linear interpolation within each triangle. ggplot() + geom_spatraster(data = CHM, aes(fill = Z)) + scale_fill_gradientn(name = &quot;Canopy height (m)&quot;, colors = height.colors(25), na.value=&quot;white&quot;) + geom_sf(data = sf::st_cast(ROI, &quot;LINESTRING&quot;)) + ggtitle(&quot;Paracou P16 4ha - CHM ALS 2023 - 1m res&quot;) + theme_classic() + coord_sf() 7.11.2 Generate a DTM (MNT) https://r-lidar.github.io/lidRbook/dtm.html l’élévation est dans la variable Z (dtm$Z) # Triangulation (généralement utilisée) dtm_tin &lt;- rasterize_terrain(ST, res = 1, algorithm = tin()) plot_dtm3d(dtm_tin, bg = &quot;black&quot;) # Invert distance weighting dtm_idw &lt;- rasterize_terrain(ST, algorithm = knnidw(k = 10L, p = 2)) plot_dtm3d(dtm_idw, bg = &quot;black&quot;) # Kriging dtm_kriging &lt;- rasterize_terrain(ST, algorithm = kriging(k = 40)) plot_dtm3d(dtm_kriging, bg = &quot;black&quot;) mnt &lt;- dtm_tin ggplot() + geom_spatraster(data = mnt, aes(fill = Z)) + scale_fill_gradientn(name = &quot;Elevation (m)&quot;, colors = terrain.colors(20), na.value=&quot;white&quot;) + theme_classic() 7.11.3 La réflectance (albedo) Calculer la réflectance apparente en ratio (albedo) Réflectance (ou Albedo) = énergie lumineuse réfléchie/énergie lumineuse incidente VP$initial_intensity &lt;- VP$Intensity # Intensité = réflectance apparente en ratio (albedo) VP$Intensity = 10^(VP$Reflectance/10) # Réflectance initialment en decibel (et selon une référence connue) "],["parallelisation.html", "Cours 8 Parallelisation", " Cours 8 Parallelisation https://rdrr.io/cran/lidR/man/lidR-parallelism.html Laz sur ordi petits car compréssés, mais dans R plusieurs giga. library(future) # chunk-based parallelism plan(multisession, workers = 2L) # chunks numbers set_lidr_threads(2L) # Nested parallelism (threads numbers) # chunks nbrs * threads numbers need to be &lt;= cores ctg &lt;- readLAScatalog(&quot;folder/&quot;) "],["c.html", "Cours 9 C++", " Cours 9 C++ Intérêt : + rapide Package Rcpp : simplifie l’intégration de code C++ dans R Inconvénients : difficile à débugger -&gt; n’y écrire que du code maitrisé et méritant cet effort Sous R : + préparation/vérification des données + traitement et la présentation des résultats. C++ : + dans un package (dossier “src”) + dans un doc C++ (.cpp) + dans RMarkdown avec insertion chunk Rcpp sourceCpp() : compilation du .cpp Créer une fonction C++ : calculer le double d’un vecteur numérique #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] NumericVector timesTwo(NumericVector x) { return x * 2; } -&gt; Une fct R du meme nom que la fct C++ s’est créée timesTwo(1:5) "],["plot-1.html", "Cours 10 Plot 10.1 A quoi faire attention 10.2 Des packages intéressants", " Cours 10 Plot 10.1 A quoi faire attention https://www.data-to-viz.com/caveats.html 10.2 Des packages intéressants Faire des graphs ggplot en clic bouton : esquisse::esquisser() Des plots tout beau prêts à publier : ggpubr "],["theme.html", "Cours 11 Theme 11.1 Types 11.2 Titres 11.3 Des couleurs 11.4 Des histoires de texte 11.5 Axes 11.6 Multi-plots 11.7 Légende 11.8 Ajouter des lignes", " Cours 11 Theme fond blanc avec lignes sans cadre : theme_minimal() + fond blanc sans lignes : theme_classic() + 11.1 Types geom_point() geom_line() geom_bar() geom_histogram(binwidth=1, fill=“#69b3a2”, color=“#e9ecef”, alpha=0.9) geom_boxplot() heatmap : ggplot(data, aes(x, y)) + geom_tile() + # heatmap geom_bin2d(bins = 50) # for counts # Points from different variables of the same dataset ggplot(data, aes(x = x)) + geom_point(aes(y = y1), color = &quot;blue&quot;) + geom_point(aes(y = y2), color = &quot;red&quot;) 11.2 Titres Titre de la figure et noms des axes : labs(title=“Figure title”, x =“x title”, y = “y title”) ou ggtitle(“Figure title”) 11.3 Des couleurs Voir les palettes : RColorBrewer::display.brewer.all() Voir palettes from ggsci R package Couleurs colorblind (daltonien) friendly : RColorBrewer::display.brewer.all(colorblindFriendly = TRUE) library(colorBlindness) availableColors() c(&quot;black&quot;, &quot;#000000&quot;,&quot;#999999&quot;, &quot;skyBlue&quot;,&quot;bluishGreen&quot;,&quot;blue&quot;, &quot;#0072B2&quot;,&quot;#56B4E9&quot;, &quot;#009E73&quot;,&quot;#D55E00&quot;, &quot;#CC79A7&quot;,&quot;vermillion&quot;, &quot;reddishPurple&quot;,&quot;#E69F00&quot;,&quot;orange&quot;,&quot;#F0E442&quot;, &quot;yellow&quot;) colorblindPalette &lt;- c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) availablePalette() # les couleurs que j&#39;aime c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;, &quot;steelblue&quot;) attribuer une couleur à chaque level scale_color_manual(values = c(&quot;1-5&quot; = &quot;#00AFBB&quot;, &quot;5-10&quot;= &quot;#CC79A7&quot;, &quot;10-20&quot;= &quot;#E7B800&quot;, &quot;&gt;20&quot; = &quot;#FC4E07&quot;)) + Gradient de couleurs : colorRampPalette(c(“blue”, “white”, “red”))(4) 4 = nbr de catégories ou scale_fill_gradient2(name = “LegendTitle”, low=“blue”, high=“red”, mid = “white”, midpoint = 0) Couleur pour des paliers : + scale_colour_steps2(name = &quot;Title&quot;, low=&quot;#00AFBB&quot;, high=&quot;#FC4E07&quot;, mid = &quot;white&quot;, midpoint = 0, breaks = seq(min(var), max(var), by = 0.02), labels = as.character(round(seq(min(var), max(var), by = 0.02), digits = 2))) utiliser une palette, donner un nombre de couleur et inverser la palette : scale_fill_gradientn(name = “Altitude”, colors = terrain.colors(45, rev=T)) # rev = T pour inverser la palette ou scale_color_gradientn() pour des points (ne pas mettre fill = mais color =) n’importe quelle palette : scale_fill_gradientn(name = “TWI”, colors = hcl.colors(12, “nom de la pallette”, rev = T)) limites au gradient de couleur : , limits = c(0,0.25) 11.4 Des histoires de texte geom_text(data = data, aes(x = x, y = y, label = Id)) Relier le label au point quand c’est pas visible : ggrepel::geom_text_repel(data = data, aes(x = x, y = y, label = Id), color = “black”, size = 2) Si positions varient selon une variable : créer un dataset avec : label &lt;- data.frame( var_x = 4, var_y = c(0.08, 0.075, 0.07), color = unique(long\\(LIDF), label = paste(&quot;MAE = &quot;,round(unique(long\\)MAE),3)) ) geom_text(data = label, aes(label = label), show.legend = F) + 11.5 Axes https://r-charts.com/ggplot2/axis/ Titre de la figure et noms des axes : labs(title=“Figure title”, x =“x title”, y = “y title”) changer l’échelle des axes (ici y): scale_y_continuous(limits = c(0, 100), breaks = c(0, 50, 100)) pour un facet_wrap : ggh4x::facetted_pos_scales( x = list( Transect == “LAY” ~ scale_x_continuous(breaks = seq(1:26)), Transect == “HOBO” ~ scale_x_continuous(breaks = seq(1:20)) ) ) + pour arrondir les valeurs des axes : scale_y_continuous(labels = scales::number_format(accuracy = 0.001)) pour transformer l’axe : scale_y_continuous(trans=“log”) pour les factor et character : scale_x_discrete(limits = as.character(seq(1, 48, by= 5))) ou breaks axe de meme longeur : coord_fixed(ratio=1) limites des axes : xlim(0,1) + ; ylim(0,60) + changer taille des titres, valeurs des axes et légende : theme(text = element_text(size = 20)) ou theme(title= element_text(size=16), axis.title= element_text(size=14), axis.text= element_text(size=16), legend.title= element_text(size=16), legend.text= element_text(size=16), strip.text.x = element_text(size=16)) + ,face=“bold” si en gras changer la couleur que du titre : theme(plot.title = element_text(colour = “red”)) Rotation des labels : theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size=7)) + 11.5.1 Temporel customiser l’échelle d’un axe temporel (date/time) : https://ggplot2.tidyverse.org/reference/scale_date.html#ref-examples un plot avec l’axe des x à échelle temporelle de toutes : les 15 minutes : + scale_x_time(breaks = hms::hms(minutes = seq(0, 3600, 15))) les semaines : + scale_x_date(date_breaks = “1 week”) mois : +scale_x_date(breaks = scales::breaks_width(“1 month”), date_labels = “%d-%m”) les 4 heures : + scale_x_time(breaks = hms::hms(hours = seq(0, 24, 4))) 11.5.2 Double axes mean(Df1$Light/Df1$Temp) # find a good coeff coeff &lt;- 2 ggplot(Df1, aes(x = Date)) + geom_line(aes(y = Temp), col = &quot;red&quot;) + geom_line(aes(y = Light/coeff), linewidth = 2) + scale_y_continuous( # Features of the first axis name = &quot;Temperature (°C)&quot;, # Add a second axis and specify its features sec.axis = sec_axis(~.*coeff, name=&quot;Light (lux)&quot;) ) + theme( axis.title.y = element_text(color = &quot;red&quot;), axis.title.y.right = element_text(color = &quot;orange&quot;) ) 11.6 Multi-plots par(mfrow = c(2,2)) 2 lignes et 2 colonnes Pour coller des plots de différentes natures : devtools::install_github(“thomasp85/patchwork”) Pour pouvoir faire un facet_wrap (plsrs graphs en 1 figure), il est plus aisé de transformer sa table de ‘Wide-format’ à ‘long-format’ (https://seananderson.ca/2013/10/19/reshape/): melt() facet avec 2 variables : facet_wrap(~ var1 + var2) Séparer un facet_wrap en plusieurs pages : facet_wrap_paginate() Echelles différentes sur un facet : scales = “free” “free_x” “free_y” or “fixed” qd on veut personnaliser : on crée un data pour un geom_blank : blank_data &lt;- data.frame(group = c(“var1”, “var1”, “var2”, “var2”), x = c(min commun, max commun, min commun, max commun), y = c(min commun, max commun, min commun, max commun)) puis geom_blank(data = blank_data, aes(x = x, y = y)) Plusieurs graphiques sur une même page : ggpubr::ggarrange(plot1, plot2, labels = c(“A”, “B”), ncol = 1, nrow = 2) 11.7 Légende geom_point(shape = &quot;circle&quot;, size = 1.5, aes(colour = &quot;Reality&quot;)) + geom_point(data = Piquets_theor, aes(x = X, y = Y, colour = &quot;Expectation&quot;), shape = &quot;circle&quot;, size = 1.5) + scale_colour_manual(values = c(&quot;Reality&quot; = &quot;forestgreen&quot;, # pour donner un nom à chaque intitulé &quot;Expectation&quot; = &quot;red&quot;), breaks=c(&quot;Reality&quot;, &quot;Expectation&quot;)) + # pour ordonner labs(color = &quot;Stakes position&quot;) # legend title Changer le titre de la légende : labs(color= “title”) pas de titre de légende : theme(legend.title=element_blank()) + cacher certaines légendes : guides(color = FALSE, fill = FALSE) pas la notation scientifique : , labels = scales::comma ou + scale_y_continuous(labels = scales::comma) ou options(scipen = 999) Position de la légende : theme(legend.position=“bottom”) Pour modifier la légende : guides(fill= guide_legend(title, theme, position, direction, override.aes = list(), nrow, ncol, reverse, order)) 11.8 Ajouter des lignes Pour ajouter une droite horizontale : abline(h=ordonnee) avec plot geom_hline(data, aes(yintercept=0, linetype=‘value’)) avec ggplot Pour ajouter une droite verticale : abline(v=abscisse) avec plot geom_vline(xintercept = value) avec ggplot Pour ajouter une droite x=y : geom_abline() des segments entre des points et des limites : geom_segment(aes(x=x, xend=x, y=y, yend=0)) ici entre chaque point et une ligne à y=0 pour la légende : scale_linetype_manual(name = ““, values =”solid”) et du texte : geom_text(data = data2, mapping = aes(x= x, y = y, label= as.character(var)), size = 3) # les colonnes de data1 et data2 impliqués dans le plot doivent etre communes "],["ajouter-une-shaded-area.html", "Cours 12 Ajouter une shaded area 12.1 Stats 12.2 Dans une boucle 12.3 Des if 12.4 En 3D 12.5 Animated plot 12.6 Télécharger dans le bon format, en bonne qualité, au bon endroit ! 12.7 En faire de l’art !", " Cours 12 Ajouter une shaded area verticale : geom_rect(aes(xmin=1990, xmax=2001,ymin=-Inf,ymax=Inf), fill=‘red’, alpha= 0.3) 12.1 Stats écart-types : geom_errorbar(aes(ymin=Mean-Sd, ymax=Mean+Sd), width=.2) ou geom_linerange() df %&gt;% ggplot(aes(x = Id)) + geom_linerange(aes(ymin= mean_Dry - se_Dry, ymax= mean_Dry + se_Dry, color = &quot;Standard error&quot;)) + # une partie des données pour créer la légende de la barre d&#39;erreur geom_linerange(aes(ymin= mean_Dry - se_Dry, ymax= mean_Dry + se_Dry, color = &quot;Dry season&quot;)) + # bar d&#39;erreur de la couleur de la donnée geom_point(aes(y = mean_Dry, colour = &quot;Dry season&quot;)) + geom_line(aes(y = mean_Dry, colour = &quot;Dry season&quot;)) + geom_linerange(aes(ymin= mean_Wet - se_Wet, ymax= mean_Wet + se_Wet, color = &quot;Wet season&quot;)) + geom_point(aes(y = mean_Wet, colour = &quot;Wet season&quot;)) + geom_line(aes(y = mean_Wet, colour = &quot;Wet season&quot;)) + scale_colour_manual(values = c(&quot;Wet season&quot; = &quot;#0072B2&quot;, &quot;Dry season&quot; = &quot;#999999&quot;, &quot;Standard error&quot; = &quot;#666666&quot;), breaks=c(&quot;Dry season&quot;, &quot;Wet season&quot;, &quot;Standard error&quot;)) + guides(colour = guide_legend(override.aes = list(linetype = c(&quot;blank&quot;, &quot;blank&quot;, &quot;solid&quot;), # autant que d&#39;entité de légende linewidth = 0.4))) Pour ajouter une courbe de tendance (régression) du nuage de points : panel.smooth(x,y) ajouter l’equation et le R2 de la régression : ggpubr::stat_cor(label.x = 0.01, label.y = 40000000) + ggpubr::stat_regline_equation(label.x = 0.01, label.y = 37000000) 12.2 Dans une boucle Pour boucler un ggplot il faut l’enlober dans un print() 12.3 Des if si qu’une commande à mettre dans le if : {if(condition) geom_hline(yintercept=15) } + Si plsrs commandes : il ne faut pas de + dans le if, mettre en liste : {if(condition) list(geom_hline(yintercept=15), geom_hline(yintercept = 13) )} + 12.4 En 3D https://www.sthda.com/english/wiki/impressive-package-for-3d-and-4d-graph-r-software-and-data-visualization 3D fixe plot3D::scatter3D() 3D pivotable : plotly::plot_ly(data, x = ~x, y = ~y, z = ~z, color=var) rgl::plot3d() 12.5 Animated plot package gganimate https://r-graph-gallery.com/animation.html plotly::plot_ly(data, x = ~x, y = ~y, z = ~z, color=var) identifier le num de lobservation sur le graphique en cliquant sur le point : identify(varx,vary) 12.6 Télécharger dans le bon format, en bonne qualité, au bon endroit ! Bonne qualité : au moins 300 dpi (72 par défaut) Quand on augmente la résolution il faut aussi augmenter la hauteur et largeur du plot You can convert inches to centimeters dividing by 2.54. avec le postscript format (.ps) on peut zoomer sans perdre de qualité : postscript() PNG : png(path, pointsize=30, width=1400, height=960, res=800) avant commande du plot &amp; dev.off() après commande du plot jpeg : jpeg(“my_plot.jpeg”, quality = 100) quality en % pdf : pdf(path, pointsize=30, width=40, height=25, paper = “A4”) avant commande du plot &amp; dev.off() après commande du plot ggsave(“mtcars.pdf”, path = , width = 25, height = 15, units = “cm”, dpi=800, bg=“white”) (ne fonctionne qu’avec des ggplots) compresser : save(objt, file = “objt.rda”, compress = “xz”) dans un pdf de plusieurs pages : sp &lt;- data$ScientificName # Define nrow and ncol for the facet n &lt;- length(unique(sp)) if(n&lt;4) {i = 1}else{ i = 4} # 4 col, 4 rows pdf(&quot;plotname.pdf&quot;, width = 15, height = 10) for(p in 1:(ceiling(length(unique(sp))/16))){ # p = page n° print( ggplot(data, aes(x=DBH)) + geom_histogram(binwidth=1, fill=&quot;#69b3a2&quot;, color=&quot;#e9ecef&quot;, alpha=0.9) + # color to have the lines of the bars ggforce::facet_wrap_paginate(vars(ScientificName, N), scales = &quot;free&quot;, ncol = min(n,4), nrow = i, page = p) ) } dev.off() # Close the final graphics device augmenter la taille des pages graphiques dans Rstudio : par(mar = c(4.1, 4.4, 4.1, 1.9)) 12.7 En faire de l’art ! https://www.data-to-art.com/ Philippe Verley, Thomas, Dominique Lamoniqua Pq : trop lent, trop de données, pas assez de RAM HPC : High Performance Computing Supercalculateurs : grappe de serveurs, grille informatique - cluster de calcul : grappe de serveurs/CPU models CPU : unité de calcul Bigmem : noeud gras/fat nodes, beaucoup de mémoire GPU : sur carte graphique (machine learning) Logiciels : mPI, OpenMP, CUDA (surtout pour le graphique) FLOPS : nbr d’opérations par seconde (puissance de calcul) ordi de bureau : 10 GFlops cluster MUSE : 3300 TFlops "],["cluster-de-calcul.html", "Cours 13 Cluster de calcul", " Cours 13 Cluster de calcul Machine HPC : le cluster noeud de login : pour préparer le calcul (salle d’attente) (ordinateur de façade) (pas lancer de calcul dessus) job scheduler : gestion des taches Plrs espaces disques : pour stocker les différents types de fichier Plsrs noeuds interconnectés Il faut connaitre le temps de calcul et l’espace de stockage, le nombre de coeur nécessaire Avantages: - fiabilité (pas biais machine, opérateur) - performance - confort (installation de logiciel) - scabilité (petit -&gt; gros) - assistance technique - pas d’entretien ni de maintenance Inconvénients : - technicité (chronophage à la mise en place) - installation des outils (version, licence, linux only) - transfert des données et réccupération des résultats - temps d’attente car on n’est pas le seul usager - noeuds de calcul pas tjrs pas performants "],["parllélisation.html", "Cours 14 Parllélisation", " Cours 14 Parllélisation pas de boucle mais plutot des fonctions et on paralélise sur les inputs pris en argument dans la boucle il stocke tout, si ce sont des fonctions, stocke que par fonction Rscript "],["meso-lr.html", "Cours 15 Meso-lr", " Cours 15 Meso-lr amap-polis@cirad.fr pour demander acces au cluster - 308 noeuds - 28 cours - 128 Go RAM "],["conteneurs-dockers.html", "Cours 16 Conteneurs (dockers)", " Cours 16 Conteneurs (dockers) syst d’exploitation virtuel (environnment) dployable sur la plupart des clusters ex : Docker, Singularity "],["tp.html", "Cours 17 TP", " Cours 17 TP Hote : muse-login.meso.umontpellier.fr user : etu-f_amap-01 mtp : f_amap-wNu port : 22 "],["rmarkdown.html", "Cours 18 Rmarkdown 18.1 Packages 18.2 Chunk 18.3 ## Quarto options 18.4 Spécifier un working directory différent de celui du fichier 18.5 Table 18.6 Figure 18.7 Equations 18.8 Des diagrammes avec Mermaid ! 18.9 Bibliographie 18.10 Références croisées 18.11 Cache", " Cours 18 Rmarkdown Reprise du cours de Sylvain Schmitt Autres cours: https://bookdown.org/yihui/rmarkdown-cookbook/ 18.1 Packages install.packages(c(&quot;rmarkdown&quot;, &quot;knitr&quot;, &quot;blogdown&quot;, &quot;tidyverse&quot;, &quot;citr&quot;)) texte ## Titre 18.1.1 Sous-titre Liste : chat chien oiseau Liste numérotée : Liste numérotée on peut laisser 1. et il met les bons numéros italique ou italique gras ou gras gras&amp;italique citation Pour passer à la ligne suivante/faire un nouveau paragraphe : finir par 2 espaces la ligne 18.2 Chunk Raccourci clavier pour générer un chunk : CTRL + ALT + I Chunk options : echo = F : ne pas voir le code eval = F : ne pas faire tourner le code include = F : ne pas afficher les sorties console results = ‘hide’ : ne pas afficher les sorties console message=FALSE : ne pas afficher les messages fig.width = 6 : taille des figures pour tout le doc : Voir documentation le reste à voir dans le petit engrenage du chunk Pour faire tourner un chunk avant tous les autres sans meme le runner : ```{r Setup} Afficher une valeur dans le texte : 1+2 = ‘r 1+2’ 18.3 ## Quarto options title: “title” author: “Vincyane Badouard” date: “2025-04-17” format: html self-contained: true # (pour partager le fichier indépendamment) theme: cosmo editor: source code-fold: true # (bouton on/off pour voir ou non les codes) execute: cache: true — 18.4 Spécifier un working directory différent de celui du fichier knitr::opts_knit$set(root.dir = &#39;chemin&#39;) 18.5 Table ##la fonction &quot;kable&quot; dans &quot;knitr&quot; knitr::kable(head(cars), #les en-têtes de &quot;cars&quot; caption = &quot;Légende.&quot;) 18.6 Figure fig.cap = \"Caption.\" pour la légende d’une figure fig.height = 8, fig.width=4 pour la taille de la figure library(tidyverse) ggplot(cars, aes(speed, dist))+ geom_point() 18.7 Equations Dans le texte : \\(\\alpha\\), \\(\\gamma = \\alpha + \\beta\\) En bande centrale : \\[Y \\sim\\mathcal N(\\mu,\\sigma)\\frac{1}{1+e}\\] \\[Y \\sim \\mathcal N(\\frac{\\mu_s}{\\beta \\times X}, \\sigma^2)\\] Voir le latex maths wiki 18.8 Des diagrammes avec Mermaid ! https://github.blog/2022-02-14-include-diagrams-markdown-files-mermaid/ https://mermaid-js.github.io/mermaid/#/ 18.9 Bibliographie La bilbliographie est lu à partir d’un fichier .bib préciser dans l’en-tête YAML et peut être généré automatiquement avec Mendeley, Endnote, etc … On peut aussi préciser le formation de citation avec un fichier .csl défini en ligne pour chaque journal. La référence [@Cochrane2003] se fait avec @ + [ + code bib + ] mais on peut utiliser l’addin citr pour le faire de manière interactive. Je recommande même de définir un raccourcit clavier personnel pour facilement effectuer une citation (par exemple CTRL + SHIFT + G). Enfin on place un titre Référence à la fin du document après lequel se placera la bibliographie mise en forme. 18.10 Références croisées Les références croisées nécessite d’utiliser le package bookdown même sans faire un livre à partir des formats documents2. Elle se font avec SLASH + @ + ref + ( + type + : + nom du chunk + ). Par exemple je fais référence à la table ?? et la figure ??. knitr::kable(head(cars), caption = &quot;Ceci est une table.&quot;) ggplot(cars, aes(speed, dist)) + geom_point() 18.11 Cache Le cache vous permet d’enregistrer les sorties d’un fragment de code pour que lors des prochaines compilation il ne soit pas recalculer. Il créé un dossier _files et _cache avec les images R des objets du chunk et les figures respectives. Attention, si votre chunk dépend d’un chunk qui est mis à jour et que vous ne précisez pas la dépendance il ne sera pas mis à jour. Utilisez l’option dependson ou nettoyez le cache pour éviter cela. "],["github.html", "Cours 19 Github", " Cours 19 Github Un-commit (reverse commit) without losing the changes : https://stackoverflow.com/questions/19859486/how-to-un-commit-last-un-pushed-git-commit-without-losing-the-changes "],["bookdown.html", "Cours 20 Bookdown 20.1 Fichiers particuliers", " Cours 20 Bookdown R proj particulier pour le bookdown 20.1 Fichiers particuliers index.rmd : fichier pour indiquer le header de ts les fichiers, écrit la “préface” _bookdown.yml : def construction livre book_filename: &quot;My_cours&quot; delete_merged_file: true # supprimer le fichier intermediaire de la compilation language: ui: chapter_name: &quot;Cours &quot; output_dir: &quot;docs&quot; # dossier de stockage de la compilation _output.yml : def construction des sorties bookdown::gitbook: config: toc: before: | &lt;li&gt;&lt;a href=&quot;./&quot;&gt;My_cours&lt;/a&gt;&lt;/li&gt; after: | # lien &quot;source&quot; vers le dépot Github &lt;li&gt;&lt;a href=&quot;https://github.com/VincyaneBadouard/My_cours&quot; target=&quot;blank&quot;&gt;Source&lt;/a&gt;&lt;/li&gt; collapse: section # pour ne pas afficher les sous-parties ds le menu deroulant includes: in_header: hypothesis.html # permet les comentaires dans le doc (open review) before_body: open_review_block.html scroll_hillight: yes # mise en couleur qd on est dans la partie "],["intégration-continue.html", "Cours 21 Intégration continue 21.1 Travis 21.2 Codecov", " Cours 21 Intégration continue Intégration continue = confier à un service externe certaines taches (tests, production de docs, tricot) pour limiter la perte de temps. 21.1 Travis nécessaire d’ouvrir un compte sur le site possible de s’authentifier avec son compte GitHub Lien Travis-github : clé privée PAT (Personal Access Token) Créer un jeton : Settings &gt; Developer settings &gt; Personal access tokens &gt; Generate new token &gt; décrire “Travis” et donner l’autorisation “repo” &gt; Generate token et enregistrer le jeton qqpart, sinon c’est perdu. Mon jeton déjà créé : 2c731e9c305bb450d691bdea233ede3e7f7b1d88 a ne pas perdre Activation du dépôt: Sur le site de Travis &gt; settings &gt; settings du repository &gt; Name : GITHUB_TOKEN, Value : &gt; add La liste des dépôts GitHub est présentée. Pour en activer un, cliquer sur l’interrupteur gris à côté de son nom 21.1.1 Script de contrôle de Travis fichier: .travis.yml. 21.2 Codecov Evalue la proportion testé du code 1) nécessaire d’ouvrir un compte sur le site 2) possible de s’authentifier avec son compte GitHub "],["function-creation.html", "Cours 22 Function creation 22.1 Étapes de développement : 22.2 Structure : 22.3 Nommer ses arguments 22.4 Valeurs par défaut 22.5 évaluation de la fonction 22.6 Vérification de la classe des arguments 22.7 L’argument ‘fun’ 22.8 L’argument ‘…’ 22.9 Types de fonction 22.10 Retourner un résultat 22.11 Fonctions de message et d’arrêt de fonction 22.12 Interaction avec l’utilisateur 22.13 Iteration 22.14 Parallelisation 22.15 Choses à savoir 22.16 Débogage tips 22.17 Où écrire ses fcts ? 22.18 Comment les utiliser ? 22.19 Plusieurs fonctions internes - passation d’infos", " Cours 22 Function creation 22.1 Étapes de développement : Planifier le travail (pas de programmation encore) : définir clairement la tâche à accomplir par la fonction et la sortie qu’elle doit produire, prévoir les étapes à suivre afin d’effectuer cette tâche, identifier les arguments devant être fournis en entrée à la fonction. Développer le corps de la fonction Écrire le programme par étapes, d’abord sans former la fonction, en commentant bien le code et en travaillant sur des mini-données test. Pour chaque petite étape ou sous-tâche, tester interactivement si le programme produit le résultat escompté (tester souvent en cours de travail, ainsi il y a moins de débogage à faire). Créer la fonction à partir du programme développé. Documenter la fonction. Tester la fonction : sauvegarder nos tests et bien les structurer, car ils serviront souvent. Si nous rencontrons des comportements indésirables lors des tests, déboguer la fonction : cerner le ou les problèmes, apporter les correctifs nécessaires à la fonction (que ce soit dans son corps ou dans liste de ses arguments), adapter la documentation et les tests au besoin, faire tourner de nouveau les tests, répéter ces sous-étapes jusqu’à ce que les tests ne révèlent plus aucun problème à régler ou aucune amélioration à apporter. 22.2 Structure : nomdemafonction&lt;-function(variable1,variable2...) { #ici on met le contenu de la fonction (généralement on effectue des transformations aux variables passées en argument) return(Variabledesortie) ## il s&#39;agit du résultat que va renvoyer la fonction (non négligeable!)! c&#39;est bien le but d&#39;une fct! } #une fois la fonction créée on peut l&#39;utiliser: nomdemafonction(varA,varB,...) Une fct normalement constituée ne travaille directement que sur ses arguments et non sur les objets stockés dans la console. Tous les objets qui seront utilisés dans le script doivent avoir été créés dans le script. Les seules exceptions à cette règle sont les arguments qui eux sont définis dans l’en-tête. Exemple ## Calculer le coefficient de variation (CV) (=écart type/moyenne) d&#39;une série de valeur. cv &lt;- function(x){ ## x est un vecteur contenant une série de valeurs moy &lt;- mean(x) ## moyenne de x s &lt;- sd(x) ## ecart type de x rslt &lt;- s/moy ## calcul du CV rslt #la fonction retourne le résultat } 22.3 Nommer ses arguments Noms habituels : x, y, z: vectors. w: a vector of weights. df: a data frame. i, j: numeric indices (typically rows and columns). n: length, or number of rows. p: number of columns. Règle : Si les arguments des fonctions appelées sont donnés de la forme name = object, ils peuvent être écris dans n’importe quel ordre. Dans le cas contraire, il faut respecter l’ordre des arguments. fun1 &lt;- function(data, data.frame, graph, limit) { [function body omitted] } ## Alors la fonction peut être invoquée de plusieurs manières, par exemple: ans &lt;- fun1(d, df, TRUE, 20) ## arguments dans l&#39;ordre ans &lt;- fun1(data=d, limit=20, graph=TRUE, data.frame=df) ## arguments dans le désordre et donc nommés 22.4 Valeurs par défaut fun1 &lt;- function(data, data.frame, graph=TRUE, limit=20) { ... } #on attribue une valeur à l&#39;argument dès l&#39;écriture 22.5 évaluation de la fonction return() renvoie la dernière valeur calculée nom_de_fonction &lt;- function(arguments) { instructions return(valeur) #non négligeable! c&#39;est bien le but d&#39;une fct! } 22.6 Vérification de la classe des arguments class(x) 22.7 L’argument ‘fun’ = passer une fonction a une autre fonction en argument col_summary &lt;- function(df, fun) { out &lt;- vector(&quot;double&quot;, length(df)) for (i in seq_along(df)) { out[i] &lt;- fun(df[[i]]) } out } col_summary(df, median) col_summary(df, mean) 22.8 L’argument ‘…’ Il est possible d’utiliser le symbole ‘…’ (ellipsis) dans les paramètres d’une fct pour indiquer que tous les paramètres supplémentaires seront transmis aux autres fonctions internes. fun1 &lt;- function(data, data.frame, graph=TRUE, limit=20, ...) { [omitted statements] if (graph) par(pch=&quot;*&quot;, ...) [more omissions] } 22.9 Types de fonction 22.9.1 Appel récursif La récursivité est une démarche qui fait référence à l’objet-même de la démarche à un moment du processus (=poupée russe) Une fct peut s’appeller elle-même, tant qu’il existe une condition d’arrêt. ## Scalaire : nombre réel ## Vecteur : serie de valeurs ## Sur scalaire factorielle &lt;- function(n) { if (n==1) resultat &lt;- 1 ## scalaire booléen ## arrêt de la récursion (n=1 est la condition d&#39;arrêt) else resultat &lt;- factorielle(n-1)*n ## appel récursif (si n différent de 1) return(resultat) } ## Sur vecteur factorielle &lt;- function(n) { indice &lt;- (n == 1) ## vecteur de booléens if (all(indice)) return(n) ## arrêt de la récursion (n=1 est la condition d&#39;arrêt) n[!indice] &lt;- n[!indice]*factorielle(n[!indice] - 1) ## appel récursif (si n différent de 1) return(n) } 22.9.2 Fct anonyme dans une autre fonction outer(x, y, function(x, y) x * y^2) ## une fonction est mise en argument d&#39;une autre fonction ## [,1] [,2] [,3] ## [1,] 16 25 36 ## [2,] 32 50 72 ## [3,] 48 75 108 22.10 Retourner un résultat Une fonction retourne le résultat de la dernière expression du corps de la fct. -&gt; donc il ne faut pas que la dernère expression soit une affectation (&lt;-), sinon on ne pourra pas affecter le résultat à un objet On peut retourner un résultat spécifique, à n’importe quel endroit de la fonction avec la fct return() Si le résultat que l’on veut retourner est codé par la dernière expression, return() est inutile. “return” se met plutot à la fin d’une fct, sauf cas exeptionel (if…), car la fct n’affiche plus les msg/stop si le return s’est effectué. Lorsqu’une fonction doit retourner plusieurs résultats, il est en général préférable d’avoir recours à une liste nommée. Retourner plsrs outputs : Outputs &lt;- list(objt1 = objt1, objt2 = objt2) return(Outputs) Si une fct retourne plsrs ouputs, elle doit toujours les retourner. S’il ne sont générés que sous condition, créer un objet vide du m^me nom lorsque la condition n’est pas respectée. 22.11 Fonctions de message et d’arrêt de fonction message() = message diagnostique (juste informatif) warning() = message d’erreur (problème non rédhibitoire (ex: syntax)) stop() = message d’erreur + arrêt du code (rédhibitoire) “stop” est une fonction d’erreur, si c’est juste une question de prérequis, il faut utiliser “if” pas “stop”. Un “stop” dans une fonction interne arrête également la fonction dans laquelle elle est inscrite. “try” : essaye la fonction interne sans stopper la fonction englobante. “catch” : capture le résultat du “try” quel qu’il soit. verbose : par défaut = TRUE dans R donc si on veut tjrs tout afficher il ne sert à rien de le mentionner Pour demander confirmation à l’utilisateur : readline(“pay attention to me!(press enter to continue)”) 22.12 Interaction avec l’utilisateur fun &lt;- function() { ANSWER &lt;- readline(&quot;Are you a satisfied R user? &quot;) ## question if (substr(ANSWER, 1, 1) == &quot;n&quot;) ## check the answer cat(&quot;This is impossible. YOU LIED!\\n&quot;) ## re-prompt else cat(&quot;I knew it.\\n&quot;) } if(interactive()) fun() 22.13 Iteration https://r4ds.had.co.nz/iteration.html itérer n fois : replicate(n, expr, simplify = F) (renvoie une liste) ou purrr::rerun(n, expr) ou purrr::map(1:n, ~ expr) ou lapply(seq_len(n), function(x) fun()) ## fun0 a function ## fun1 a a function to iterate fun0 with a n argument fun1 &lt;- function(n, x) replicate(n, fun0(x = x)) fun1(n, x) set.seed(1) fun0 &lt;- function(x) as.list(matrix(c(1, 1, 1, x + rnorm(1)), nrow = 2)) n &lt;- 1000 fun0(1) library(microbenchmark) mb &lt;- microbenchmark( repl = {replicate(n, fun0(1), simplify = FALSE)}, reru = {purrr::rerun(n, fun0(1))}, map = {purrr::map(1:n, ~fun0(1))}, lap = {lapply(seq_len(n), function(x) fun0(1))}, times = 1000L ) mb mb &lt;- microbenchmark( suit = {1:n}, rep = {rep(1, n)}, seq_len = {seq_len(n)}, seq_along = {seq_along (n)}, ## le + rapide c = {c(1:n)}, times = 1000L ) mb repeat() : répète à l’infini si on ne met pas une condition et un break dedans. Boucle for ou boucle while Encapsuler une boucle dans une fonction apply family (base) : apply(), lapply(), tapply(), etc map family (purrr) (+ rapides car écrite en C): map() (makes a list), map_lgl() (logical vector), map_int() (integer vector), map_dbl() (double vector), map_chr() (character vector). Les map ne produisent que des vecteurs, pas de matrice) Peuvent apliquer des fonctions ou des formules (https://r4ds.had.co.nz/iteration.html). Les boucles for ne sont plus lentes depuis de nombreuses années. Le principal avantage de l’utilisation de fonctions comme map() n’est pas la vitesse, mais la clarté : elles rendent votre code plus facile à écrire et à lire. Modif de l’output : transposer une liste (inverser la tructure) : purrr::transpose() 22.14 Parallelisation https://nceas.github.io/oss-lessons/parallel-computing-in-r/parallel-computing-in-r.html https://bookdown.org/rdpeng/rprogdatascience/parallel-computation.html Why : for time and memory 1 processor = multiple cores = multiple computations to be executed at the same time. R is sequential and uses 1 processor Plus simple de paralléliser des taches indépendantes, 1 par coeur. 1 tâche = 1 fct Packages : parallel, future, foreach, doParallel diffèrent selon les syst d’exploit on dirait Parallel functions : - detectCores() : détecte le nbr de coeur de la machine. - mclapply() : utiliser plsrs coeurs sur un ordinateur local (= parallelized version of lapply) (pas sous windows) - makeCluster() and clusterApply() : utiliser plusieurs processeurs sur des machines locales (et distantes) - clusterExport() : à distance Sous windows on ne peut pas paralléliser en local il faut donc un cluster : - créer un cluster : cl &lt;- makePSOCKcluster(nbr de copies de R à créer) (à défaut d’avoir des coeurs il crée plsrs copies de R) - créer l’envmt du cluster : clusterExport(cl, varlist = c(“var1”, “var2”), envir = environment()) - paralléliser : parLapply(cl, seq_along(iter), myfun) - arrêter le cluster : stopCluster(cl) https://stackoverflow.com/questions/17196261/understanding-the-differences-between-mclapply-and-parlapply-in-r library(parallel) cl &lt;- makePSOCKcluster(4) ## créer un cluster myfun &lt;- function(i) { Sys.sleep(1); i } ## the function parLapply(cl, 1:8, myfun) ## parallelization stopCluster(cl) ## stop the cluster Sur des boucles : - foreach sequential operator : %do% - doParallel parallelizable operator : %dopar% ## nonparallel for loop for (i in 1:3) { print(sqrt(i)) } ## nonparallel foreach loop library(foreach) system.time({ foreach (i=1:3, .combine=c) %do% { ## .combine=c si on veut unlist l&#39;output, .combine=rbind si on veut un df sqrt(i) } }) ## doParallel = foreach parallel adaptor for the &#39;parallel&#39; package library(doParallel) numCores &lt;- parallel::detectCores(logical = FALSE) ## FALSE to have the physical cores system.time({ registerDoParallel(numCores) foreach (i=1:3) %dopar% { sqrt(i) } }) # with foreach cores &lt;- detectCores() j = length(lases) # Create clusters cl &lt;- parallel::makeCluster(cores) doSNOW::registerDoSNOW(cl) # Progressbar if(interactive()) pb &lt;- txtProgressBar(max = j, style = 3) # if(interactive()) pour qu’elles n’apparaissent pas dans les documents markdown progress &lt;- function(n) if(interactive()) setTxtProgressBar(pb, n) opts &lt;- list(progress = progress) # The loop output &lt;- foreach::foreach( i=1:j, .packages = c(&quot;magrittr&quot;), .options.snow = opts ) %dopar% { print(i) # to debug las_to_dem(lases[i], las_path = las_path, mnt_path = mnt_path) } # close the progressbar close(pb) # close the cluster stopCluster(cl) When you’re done, clean up the cluster : stopImplicitCluster() 22.15 Choses à savoir Une fct a un environnement propre (ses var sont locales et ne vont pas dans l’envmt de travail): ainsi pas de conflit. Eviter donc d’utiliser des objets provenant de l’envmt de travail. Pour cela le mieux est de garder son environnement global tjrs vide (vider tt l’envrmt global : rm(list = ls())) On peut définir une fonction à l’intérieur (= fonction interne) d’une autre fonction. Cette fonction sera locale à la fonction dans laquelle elle est définie. 22.16 Débogage tips Lorsque Browse[1]&gt; s’affiche dans la console, on peut écrire des commandes utilisant l’environnement de la fonction en bug, et donc la travailler. Un booléen T/F ne marche pas si des NA existe Lorsqu’une fct ne retourne pas le résultat attendu, placer des commandes print à l’intérieur de la fct, afin de suivre les valeurs prises par les différentes variables. On peut même écrire print(1) après la 1ère ligne, print(2) après la 2eme, ainsi de suite pour savoir à quelle ligne se trouve l’erreur. Quand ce qui précède ne fonctionne pas, ne reste souvent plus qu’à exécuter manuellement la fonction : définir dans l’espace de travail tous les arguments de la fonction, puis exécuter le corps de la fonction ligne par ligne. La vérification du résultat de chaque ligne permet généralement de retrouver la ou les expressions qui causent problème. ou masquez avec le « ## » toutes les commandes que vous venez d’éditer ou rajouter, sauf la 1ere, Sauvez, et testez le fichier afin de voir si cette ligne est correcte. Si « source » fonctionne, enlevez le prochain dièse. N’oubliez pas de sauver et de « sourcer » à chaque test. 22.17 Où écrire ses fcts ? Dans un fichier .R indépendant nom de fichier = nom de la fonction 22.18 Comment les utiliser ? Enregistrer le code de la fct dans l’envmt : source(“nomfct/file.R”) 22.19 Plusieurs fonctions internes - passation d’infos ## Main : fonction englobante main &lt;- function(inventory){ inventory.volume &lt;- getvolume(inventory) rm(inventory) inventory.final &lt;- filtervolume(patate = inventory.volume) rm(inventory.volume) return(inventory.final) } ## Fonctions internes getvolume &lt;- function(inventory){ inventory &lt;- inventory %&gt;% mutate(v = 2*d) return(inventory) } filtervolume &lt;- function(patate){ poireau &lt;- patate %&gt;% filter(v &gt; 0) return(poireau) } ## test inventory &lt;- data.frame(id = 1:10, d = rnorm(10)) main(inventory) "],["package-creation.html", "Cours 23 Package creation 23.1 References 23.2 Les packages R nécessaires 23.3 Creation du package 23.4 Tips de codeur 23.5 Eviter les messages d’erreurs 23.6 Commandes 23.7 Importation de fonctions 23.8 Créer des classes pour ses objets en plus de ceux de base 23.9 Créer une “méthode générique” 23.10 tidyverse 23.11 Vignette 23.12 Site du package : Pkgdown 23.13 Des petits messages à l’utilisateur 23.14 C++ 23.15 Biblio 23.16 Where stock my data in my package 23.17 Charger ces data 23.18 Tests 23.19 Un package pour tous 23.20 Mettre le package sur CRAN 23.21 Debugger 23.22 Versioning 23.23 Renommer le package 23.24 Installer le package 23.25 Générer la citation du package", " Cours 23 Package creation 23.1 References The original bible : https://r-pkgs.org/index.html en francais : https://www.imo.universite-paris-saclay.fr/~goude/Materials/ProjetMLF/editer_package_R.html Eric Marcon’s cours : https://ericmarcon.github.io/Cours-travailleR/ 23.2 Les packages R nécessaires Télécharger des packages d’aide à l’organisation et structure, nécessaire à l’écriture d’un package : ## install.packages(c(&quot;usethis&quot;, &quot;roxygen2&quot;, &quot;devtools&quot;, &quot;testthat&quot;, &quot;covr&quot;, &quot;goodpractice&quot;)) ## ## library(usethis) #automatise la création des dossiers, vignette, site ## ## library(roxygen2) #permet d’automatiser la documentation obligatoire des packages ## ## library(devtools) #boîte à outils, permettant notamment de construire et tester les packages ## # roxygen2::roxygenise()#refresh de la documentation ## ## library(testthat) # Create test units ## ## library(covr) # Test coverage report() ## ## library(available) #vérifier la disponibilité du nom de votre package ## ## # available::available(&quot;LoggingLab&quot;) # TreeData ## ## library(goodpractice) # Bonnes pratiques dans la construction de package (functions and syntax to avoid, package structure, code complexity, code formatting) ## gp(system.file(package = &quot;Maria&quot;)) #trouve les mauvaises pratiques de notre package ## si on avait une ancienne version de withr pour rstan ## remove.packages(&quot;withr&quot;) ## install.packages(&quot;withr&quot;) 23.3 Creation du package New Project &gt; New Directory &gt; R package using devtools… Nom du package : pas d’espace, pas d’underscore, ne pas commencer par un chiffre et en minuscule Build &gt; Install and restart construit et charge le package dans R 23.4 Tips de codeur Mesurer le temps d’exécution d’un code : system.time ou microbenchmark (fct1,fct2) pour des codes trés courts. Ce sont les médianes des temps de calcul qu’il faut comparer. Détacher un package de l’environnement : unloadNamespace() Définitions “Charger/Attacher” un package : pkg::fct = pas attaché library(pkg) = attaché dans un cas comme dans l’autre, le package est chargé (load) : on l’utilise. Trouver les non ASCII characters : tools::showNonASCIIfile(“R/zzz.R”) les lignes doivent faire 80 charactères max 23.5 Eviter les messages d’erreurs Ne pas sauter de lignes dans “DESCRIPTION” file 23.6 Commandes Build &gt; Install and restart permet de tester des modifications de code check ou devtools::check() permet de vérifier son code (màj de la documentation (dossier “man”), tests sur le code, renvoie erreurs et avertissements) checker uniquement les exemples : devtools::run_examples() git branch -d main dans la console permet de supprimer la branche “main” de git Documentation-exporter une fonction : placer le curseur dans la fonction et appeler le menu “Code &gt; Insert Roxygen Skeleton” (ou CTRL + ALT + SHIFT + R): roxygen2 la déclarera dans “NAMESPACE” après un “check” raccourcir les lignes du squelette : ctrl + shift + / 23.6.1 Comprendre le Skeleton = la page d’aide de la fonction ! Roxygen tags : @include : mentionne des fichiers .R appelés dans le fichier @import : importer l’intégralité des fonctions du package (seulement si on utilise bcp de fcts du package (ex: ggplot2) @importFrom package fct : aller chercher une fonction dans un package (à privilégier) @param : y décrire le paramètre et citér sa classe @section : créer une nouvelle section @return décrire le résultat de la fonction @export déclare que la fonction est exportée : elle sera donc utilisable dans l’environnement de travail @examples : - Ne pas runner l’ex durant les tests : Pour un exemple qu’on ne veut pas encore “checké” Hyperliens : nom affiché Si on n’en a marre de se répéter : + Cross-link documentation files : @seealso and @family. Inherit (héritée) documentation from another topic : @inherit, @inheritParams, and @inheritSection. Document multiple functions in the same topic with @describeIn or @rdname. Créer une fct pour les écrire !! https://cran.r-project.org/web/packages/roxygen2/vignettes/rd.html (Evaluating arbitrary code) 23.6.2 R folfer Contiendra 1 ou plsrs fichiers .R contenant nos fonctions. 1 fichier peut contenir 1 fonction ou on peut y rassembler plsrs fonctions regroupables 23.6.3 nom-package.R file = page d’aide du package (=1er bloc) (visible avec ?nomdupackage) = commentaires pour roxygen2 = supplément de la vignette C’est aussi là qu’on renseigne les global variables La générer : usethis::use_package_doc() #&#39; TreeData-package #&#39; #&#39; Forest Inventories Harmonization &amp; Correction #&#39; #&#39; @name TreeData #&#39; @docType package #&#39; #&#39; @section TreeData functions: #&#39; RequiredFormat #&#39; #&#39; @keywords internal &quot;_PACKAGE&quot; ## usethis namespace: start ### quiets concerns of R CMD check &quot;no visible binding for global variables&quot; utils::globalVariables(c(&quot;.&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) ## usethis namespace: end NULL 23.6.4 NAMESPACE file “gestion des noms des objets” = interaction du package avec le monde exterieur : importation d’autres fonctions/packages, exportation de nos fonctions. 23.6.5 DESCRIPTION file Depends: liste des packages d’origine de chaque générique Depends: R (&gt;= 2.10), ggplot2, graphics Les alinéas sont nécessaires Tout package utilisé (dépendances) doit être mentionné dans le fichier DESCRIPTION, section ‘Imports’ Attention : les dépendences appartenant à R core (ex: stats, parallel) doivent être de meme version munimum que celle de R demandée. 23.6.6 .Rbuildignore = fichiers dans le package pour le constructeur, mais qui ne seront pas chargés avec le package pour l’utilisateur. 23.7 Importation de fonctions le package d’appartenance de la fonction doit obligatoirement être déclaré : DESCRIPTION &lt; Imports: nom package, nom package Le package tydiverse ne doit pas être importé car trés changeant Import du pipe : usethis::use_pipe() Pour ne pas le faire à la main (préférable): usethis::use_package(“stats”) Importer la fonction dans nompackage.R : #’ @importFrom package fct dans le squelette de la fct (fichier.R) la fonction importée doit être trouvable dans l’environnement du package en création : la meilleure pratique est de qualifier systématiquement les fonctions d’autres packages dans le code avec la syntaxe package::fonction() (Sylvain dit que non) 23.8 Créer des classes pour ses objets en plus de ceux de base class(x) &lt;- “MyClass” ou class(y) &lt;- c(“MyClass”, class(y)) si on veut ajouter une classe aux classes de l’objet déjà existantes 23.9 Créer une “méthode générique” méthode générique = modèle de fonction, sans code, à décliner selon la classe d’objet à traiter (ex : “plot”). Une méthode générique est plus lente en calcul que ses fonctions dérivées qui sont spécialisées à une classe d’objet. meth_generique &lt;- function(x, ...) { UseMethod(&quot;meth_generique&quot;) } ## Fonction dérivée : meth_generique.class &lt;- function(x, ...) { return(x * 3L) #le suffixe L signifiant que 3 doit être compris comme un entier. } ##créer une fonction dérivée à partir d&#39;une fonction dérivée déjà existante print.multiple &lt;- function(x, ...) { print.default(x$y)#n&#39;afficher que le resultats y } Sa signature : ensemble de ses arguments Les fonctions dérivées de cette méthode devront obligatoirement avoir les mêmes arguments dans le même ordre et pourront seulement ajouter des arguments supplémentaires avant “…” (qui est obligatoire). 1er argument : “x”. Dépend de la classe de l’objet 23.10 tidyverse Tout package doit être compatible avec le tydiverse. pour permettre l’utilisation de pipelines, l’argument principal = le premier fcts qui transforment des données doivent accepter un dataframe ou un tibble comme premier argument et retourner un objet du même format les méthodes plot() doivent être doublées de méthodes autoplot() avec les mêmes arguments qui produisent le même graphique qu’avec ggplot2. autoplot() : permet de visualiser tt type d’objet, meilleur que ggplot2. aes() devient aes_() et ajouter un ~ devant les noms des variables. R CMD CHECK NOTE “no visible binding for global variable” : mettre @importFrom rlang .data 23.11 Vignette Création usethis::use_vignette(“nomdupackage”) Les packages utilisés par la vignette doivent apparaitre dans DESCRIPTION C’est un rmd donc ne pas oublier de kniter souvent Contenu : + library(monpackage) dans le premier chunk. + introduction à l’utilisation du package + chunk avec un exemple d’utilisation Compilation : Pour compiler la vignette (Build ne le fait pas) (et la voir dans la page d’aide du package): Pour mettre une copie de la vignette dans /doc (durant le dvp) : devtools::build_vignettes() On peut détruire ce qui est dans doc/ après usage. Le dossier est déclaré dans .Rbuildignore et .gitignore. Push sur Github puis devtools::install_github(“DeveloperName/PackageName”, build_vignettes = TRUE) Cette methode (installer le package à partir de sa source) le permet aussi, mais plus triviale : https://www.r-bloggers.com/2020/05/how-to-add-a-vignette-to-a-package-in-rstudio/ Pour que le check ne construise pas la vignette : –no-build-vignettes dans options &gt; check et build pckg Si le build renvoie une erreur relative à qpdf : il faut installer les Rtools et au bon endroit (voir 1.1.2 Rtools TravailleR d’Eric Marcon) 23.12 Site du package : Pkgdown https://pkgdown.r-lib.org/articles/pkgdown.html ! le dépot doit être public ! Création site du package (dossier “docs”): usethis::use_pkgdown() puis pkgdown::build_site() Use GitHub actions to automatically build and publish the site every time you make a change usethis::use_pkgdown_github_pages() Configures a GitHub Action to automatically build the pkgdown site and deploy it via GitHub Pages : usethis::use_github_action(“pkgdown”) Home page : index.md or README.md Reference : liens vers les pages d’aide de toutes les fcts du package. On peut les organiser voir le lien. Articles : vignettes News : NEWS.md Customise your site: https://pkgdown.r-lib.org/articles/customise.html 23.13 Des petits messages à l’utilisateur Afficher des messages informatifs : cat() ex : cat(“multiplied by”, object$times, “is:”) donne “multiplied by 2 is:” 23.14 C++ un C++ file dans dossier “src” un .gitignore dans ce même dossier, dans lequel on met : “# C binaries src/.o src/.so src/*.dll” 23.15 Biblio Les réf biblio (sauf celles des .rmd (vignette, site) sont gérées automatiquement avec Rdpack et roxygen2 De notre zotero/mendeley vers : REFERENCES.bib dans le dossier inst Ajouter dans le fichier DESCRIPTION : RdMacros: Rdpack Imports: Rdpack Comment citer : citation(packagename) Biblio de la vignette : + créer un fichier bib + le déclarer dans l’entête + citer avec @citation. On peut éviter de multiplier les fichiers bib en utilisant celui de RdPack qui est inst/REFERENCES.bib : dans la vignette, tu déclares le chemin relatif : bibliography: ../inst/REFERENCES.bib 23.16 Where stock my data in my package 23.16.1 Testing data: Just for me : in inst/extdata (‘inst’ pour ‘installed’) : external data (private) (en csv) -&gt; no need to document For the user (and me) : in “data” : internal data (= deliver with the package) (public) -&gt; need to document 23.16.2 Default data: Dataset : in “data” : internal data (= deliver with the package) (public) -&gt; need to document Valeurs par default d’arguments de fct : Fct d’arguments : si on veut passer des valeurs par défaut dans une fct, on peut les proposer en arguments de celle-ci. S’il y en a bcp, on peut créer une autre fct qui aura pour arguments ces valeurs par défaut et qui créé une liste de celles-ci. Cette dernière fct sera entrée comme argument de la 1ère. Ainsi n’importe quelle valeur peut être modifiée sans avoir à tout réécrire. fun1(A, B, arg = fun2(arg10 = FALSE)) Liste dans data : on peut aussi juste créer la liste et en faire un data que l’utilisateur loadera, modifira, et injectera dans la fct en argument, mais ça lui demande donc du travail préliminaire. “Global options” in zzz file 23.16.2.1 “Global options” Where write options: in zzz file (in R folder), as a named list or vector (eg. pars &lt;- list(track_size = 2, method = “EFI”). Then you called pars$method.). Options = an object (here a list/vector) zzz file : on y met généralement les actions devant se réaliser au chargement du package. (https://cran.r-project.org/web/packages/GlobalOptions/vignettes/GlobalOptions.html#session_info) (https://github.com/mojaveazure/seurat-disk/blob/master/R/zzz.R) Inconvénients des options : on ne peut pas appliquer de apply sur différentes valeurs choisies d’une même option. Documenter les “options” du package : dans un fichier .R dons le dossier R (regarder ?rstan_options pour inspiration) Modification des options par l’utilisateur : options() http://www.endmemo.com/r/options.php If you modify global options() or graphics par(), save the old values and reset when you’re done: old &lt;- options(optionsname = defaultvalue) save the old values on.exit(options(old), add = TRUE) reset when you’re done To access the value of a single option, one should use, e.g., getOption(“width”) rather than options(“width”) which is a list of length one. 23.16.3 Exported data (in data/) accessible à l’utilisateur ! usethis::use_data(data1, data2) : crée un fichier (.rda) pour chaque jeu de données/variable, dans le dossier data pas conseillé de prendre un autre format de fichier, celui-ci est pertinent (rapide et léger). DESCRIPTION file : LazyData: true -&gt; data dispo dès le chargement du package, ne chargeront qu’au besoin. comprésser un fichier : bzip2, gzip ou xz si le fichier data est une version transformée d’une base de donnée, il est conseillé de mettre le code de transformation dans data-raw/ grace à : usethis::use_data_raw(“nom du data”) 23.16.4 Documenter ces données exportées Où : dossier ‘R’ nommé ‘data.R’ roxygen squeleton : ##&#39; Data title ##&#39; ##&#39; Data description ##&#39; ##&#39; @format A data frame with X rows and Z variables: ##&#39; \\describe{ ##&#39; \\item{var1}{description, in units} ##&#39; \\item{var2}{description, in units} ##&#39; ... ##&#39; } ##&#39; @source \\url{} commande d&#39;acquisition des données &quot;name&quot; 23.16.5 Internal data for a function (in R/sysdata.rda) Pour : stocker une base de données analysée/pré-calculée, dont a besoin une fonction. non-accessible à l’utilisateur ! -&gt; pas à être documenté mise dans R/sysdata.rda grace à usethis::use_data(, internal = TRUE) si le fichier data est une version transformée d’une base de donnée, il est conseillé de mettre le code de transformation dans data-raw/ grace à : usethis::use_data_raw() 23.17 Charger ces data Quand : pour les exemples et les tests, pas dans les fonctions ! Les fonctions ne manipulent que leurs propres objets (en 1er lieu leurs arguments). (Le mieux c’est de garder son environnement global vide lors de la création de la fonction.) system.file(“dossier”, “fichier”, package = “nompackage”) : donne le chemin vers un fichier d’un package, pour tout système d’exploitation. load() : pour charcher les .rda. Ex : load(system.file(“extdata”, “BrokenParacou6_2016.rda”, package = “Maria”)) read_csv : pour charcher les csv data(nomdufichier) : charge le fichier s’il est dans le dossier “data” du package 23.18 Tests Créer un dossier “tests” : usethis::use_testthat() écrire des tests à nos fonctions dans tests/testthat : usethis::use_test(name = “nomdelafct”) The test name should complete the sentence “Test that …”. ## test_that(&quot;Math works&quot;, { ## expect_equal(1 + 1, 2) ## expect_equal(1 + 2, 3) ## expect_equal(1 + 3, 4) ## }) lancer tous les tests : devtools::test() (Ctrl + Shift + T) Each test is run in its own environment and is self-contained. chaque test s’applique sur les outputs de la fct testée, pas sur les objets internes dans les test, ne charger la function qu’1 fois si possible pour limiter le temps de calcul (limité sur CRAN) Ne pas afficher les messages renvoyées par les fcts testées dans les tests : suppressMessages(fun()) package “teststat” : vérification que la fonction fonctionne et renvoie les valeurs qu’on veut package “covr” (juste copier report() dans la console) : qui dit quelle proportion du code est sous unités de tests (couverture) Devtools (https://testthat.r-lib.org/reference/index.html): expect_equal() is equal within small numerical tolerance? expect_identical() is exactly equal? expect_match() matches specified string or regular expect_message() displays specified message? expect_warning() displays specified warning? expect_error() throws specified error? expect_type() output inherits from certain class? expect_false() returns FALSE? expect_true() returns TRUE? logical-expectations() : Does code return TRUE or FALSE? expect_null() : Does code return NULL? expect_length : code return a vector with the specified length? equality-expectations() : code return the expected value? expect_vector() : Does code return a vector with the expected size and/or prototype? expect_named() : Does code return a vector with (given) names? comparison-expectations() : Does code return a number greater/less than the expected value? expect_setequal() : Does code return a vector containing the expected values? expect_output() : Does code print output to the console? try_again() : Try evaluating an expressing multiple times until it succeeds. make_expectation() : Make an equality test. test_examples() : Test package examples these functions have two arguments: the 1st is the actual result, the 2nd is what you expect. 23.19 Un package pour tous Pour tout système d’exploitation : mentionnés dans le yaml de githubactions : il fait les tests pour chacun d’eux 23.20 Mettre le package sur CRAN Version number : changer le numéro en version (plus en dévelopement = sans le 9000) (major.minor.patch) (1.0.0) Test environments : tous les tests doivent êtres réussis sur min 2 syst d’exploitation 0 notes/warning/error Backward compatibility : pas important pour la première version mais important pour la suite si le package évolue Submission : je déconseille d’utiliser devtools::release() mais plutôt de le faire à la main pour bien contrôler les étapes comments.md : très important Dépendances : toutes les dépendances hors base doivent être inclue, y compris des packages core comme stat ou utils (à discuter). Prepare for next version : je pense que c’est super important et que la plupart des gens ne le font pas. En gros une fois que CRAN a dit oui tu es contente et tu vas vouloir t’arrêter là. Sauf qu’un package est souvent destiné à évoluer et cette petite étape te permettra juste de réattaquer facilement, donc je trouve ça vitale. 23.20.1 Les tests Githubactions : pour un contrôle standard :usethis::use_github_action_check_standard() -&gt; R-CMD-check.yaml creation https://youtu.be/K4x-uqLl_m4 Si les Github Actions Windows ne fctnent pas (ne trouve pas les fcts du package), alors que ça marche sur les autres OS, dans le R-CMD-check.yaml: # https://github.com/privefl/minipkg/commit/50224bcf5f1fd4e30a8c15d2a10534fb247fef4b - name: Install dependencies run: Rscript -e &quot;install.packages(&#39;devtools&#39;)&quot; -e &quot;devtools::install(dependencies = TRUE)&quot; Codecov : test-coverage.yaml usethis::use_github_action(“test-coverage”) (https://github.com/r-lib/actions/tree/master/examples#test-coverage-workflow) (créer un “secret” pour le token sur github si le dépot est privé) Il faut créer un compte codecov afficher le rapport de couverture du package : covr::report() 23.21 Debugger tapper ligne par ligne dans la console la partie qui coince Lorsque Browse[1]&gt; s’affiche dans la console, on peut écrire des commandes utilisant l’environnement de la fonction en bug, et donc la travailler. 23.22 Versioning https://r-pkgs.org/release.html#release-version major.minor.patch.9000 .9000 only for dvlmpt version major, nimor, patch for released version il peut y avoir des versions de dvlp entre released versions For development version: 9000 +1 / commit for example. You can choose your unit For released version: patch : just debuging, no addings minor : debuging, addings, with backward compatibility (same functions and arguments names) major : substantive changes, not backward compatible, affect many users 23.23 Renommer le package https://docs.github.com/en/repositories/creating-and-managing-repositories/renaming-a-repository + Renommer le dépôt Github puis + $ git remote set-url origin new_url in the shell + ctrl + shift + F remplacer l’ancien nom par le nouveau dans tous le package 23.24 Installer le package Dépot public : devtools::install_github(&quot;DeveloperName/PackageName&quot;, build_vignettes = TRUE) Dépot privé : Create an access token in: https://github.com/settings/tokens devtools::install_github(&quot;user/repo&quot; ,ref=&quot;master&quot; ,auth_token = &quot;tokenstring&quot; # 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 ) 23.25 Générer la citation du package mettre un onglet Date dans le fichier DESCRIPTION, y écrire une date dans le format yyyy-mm-dd et runer citation(“nom du package”) "],["shiny-cours.html", "Cours 24 Shiny cours 24.1 Packages 24.2 Tips codeur 24.3 Shiny: 24.4 Structurer l’appli 24.5 Créer l’interface 24.6 Gérer les intéractions 24.7 Enrichir l’appli 24.8 Autres packages 24.9 Mise en ligne et partage de l’appli 24.10 Module 24.11 Autres sources cours", " Cours 24 Shiny cours 24.1 Packages library(shiny) # Web Application Framework for R library(bslib) # aesthetic, thème et feuilles de style ## bslib::bootswatch_themes() # pour voir ts les thèmes library(shinydashboard) ## library(bs4Dash) # dashboard aussi. restart R session si on passe d&#39;un pkg à l&#39;autre (dont mode jour/nuit mais mal fait) library(shinyjs) # Improve the User Experience library(dplyr) library(ggplot2) library(thematic) # Aesthetic, Unified and Automatic &#39;Theming&#39; of &#39;ggplot2&#39;, &#39;lattice&#39;, and &#39;base&#39; R Graphics library(DT) # Interactive table library(leaflet) # Interactive Web Maps library(plotly) # Interactive Web Graphics 24.2 Tips codeur à la fin d’une grosse parenthèse/accolade préciser de quoi c’est la fin #### titre partie #### ou tirets ou égales pour faire des parties déroulantes dans le code 24.3 Shiny: Package gratuit dvp par Rstudio donne à l’utilisateur une page web à consulter Ne supporte que l’encodage UTF-8 ce qu’on peut y mettre : - curseurs - cases à cocher - graphiques - cartes intéractives (changer le fond) - changer la langue - process automatique -&gt; vidéo - arbres décisionnaires R ne fait qu’1 tache à la fois donc Shiny aussi, mais il existe des moyens de palier à ça. R -&gt; Shiny -&gt; Bootstrap -&gt; HTML/CSS/JavaScript -&gt; Navigateurs internets structurer l’appli créer l’interface gérer les intéractions enrichir l’appli mise en ligne et partage de l’appli 24.4 Structurer l’appli créer un dossier ac le nom de l’appli 2 fichiers : ui.R et server.R enregistrés en UTF-8 ui.R : dont fct fluidPage() (mise en forme de l’interface) server.R : dont fct server &lt;- function(input, output, session) global.R (optionnel, même dossier) : chargement de variables dans envrment global de l’utilisateur Il est possible de mettre ces 3 codes dans un même fichier. Lancement de l’appli : bouton “Run App” ou shiny::runApp (“dossierdes2fichiers”) Appuyer sur STOP qd fini 24.5 Créer l’interface ui.R et sa fct créent l’interface graphique Le code R dans ui est converti en HTML (on peut aussi mettre du HTML dans ui (HTML())) sous-dossier www avec (non aacessibles sous code, uniqueùment sous ap): - image.png - style.css - script.js Formater du texte (fcts inspirée du HTML) à mettre dans fluidPage(): p() : paragraphe h1~h6() : 1er niv d’en-tête à 6ème niv *a(href =, target = “_blank)* : lien hypertexte, target = “_blank permet d’ouvrir dans un nouvel onglet br() : retour à la ligne code() : Police code strong() : gras em() : italique img(src = image dans www) : insérer une image , à la fin de chaque ligne dans ces fonction (sépare les args) clic droit sur la page web pour en voir le code de mise ne forme en html. 24.5.1 Mise en page Page titlePanel (titre) Layout side bar panel main panel tapPanel1,2,3 Mettre des onglets : tabsetPanel() : conteneur dedans 1er onglet : tabpanel(), 2ème onglet : tabpanel(), etc Page barre latérale et panneau principal, 1/3-2/3 : sidebarLayout() (conteneur) 24.5.2 INPUTS &amp; OUPUTS Répertoriés ici: https://shiny.rstudio.com/gallery/widget-gallery.html Dans ui : INPUT + actionButton : bouton + actionLink : bouton + checkboxInput : entrée numérique + checkboxGroupInput : groupes de cases à cocher + dateInput : date avec propositions + dateRangeInput : période OUTPUT + plotOutput : graphique + imageOutput : image + tableOutput : print tableau + dataTable : tableau optimisé (interactif: affichage et recherche) + text + verbatim : format console R + html/uiOutput : ui dynamique (faire apparaitre des choses au départ caché) 24.6 Gérer les intéractions Sur ui: un identifiant (inputId =) unique par input/output et parlant label, choices seront visibles sur l’app selected : valeur selectionnée par dédaut Sur server.R : output\\(idouput &lt;- renderText(){input\\)idinput} c’est l’objet calulé à la dernière ligne qui est utilisé pour mettre à jour l’outpout 1 fct/output même pour un même input on peut mettre un print avant la der ligne de code pour afficher le rslt dans console un input répétitif peut être mis sous forme de fct et mis soit dans servers soit dans global. Réaction réactive : la fct reactive permet de ne pas recalculer même input à chaque output (comme un cache) (diapo 46) Chaine de réactivité : Reactive value -&gt; Reactive expression -&gt; Rendu &amp; output ou Observer input = reactive value + élmt graphique (cases, surseur etc) Reactive value avec reactiveValues(result = NULL) (voir server de calculatrice) Reactive expression = calcul lourd, extract données, mise en cache pour plsrs render/oberver. Rendu &amp; output = mise à j d’un output avec un nouvel objet. Observer = gestion fine de l’interface : gestion d’evnment (click bouton), gestion de proxy. Une expression réactive peut en observer une autre expression réactive (propagation) Fonctions : reactiveValues reactive eventReactive : réagit à un évenemnt mis en 1er argument observe observeEvent isolate pour ne pas appliquer un code sans l’activation d’un code précédent (voir server calculatrice). invalidateLater : ré-exécution d’un observateur/render/expr rel après un certain laps de temps debounce/throttle 24.7 Enrichir l’appli 24.7.1 Leaflet cartographie (représentation spatiale) dynamique Utilise le pipe choisir le fond de carte voir code diapo ~devant les vars de data Définir le zoom initial voir code diapo ajouter des points spécifiques (markers) voir code diapo personaliser les points (description des markers) voir code diapo Ajouter des formes rayon en mètres voir code diapo Ajouter une légende voir code diapo Ajouter une échelle et la possibilité de faire des mesures voir code diapo Après un render pour initialiser, un proxy, encapsulé dans un observe, pour ne pas recalculer le fond de carte à chaque modif. L’objet à mettre à a préalablement été construit dans un render Des proxy existent pour leaflet, DT, plotly 24.7.2 Charger/enregistrer des fichiers Il faut mettre une limite max de gigas pour protéger le serveur. 24.7.2.1 Charger Fichiers fournis par l’utilisateur : input Dans ui : fileInput(inputId = “fiMyFiles”, multiple = TRUE,…) Dans server : diapo p54 package shinyFiles pour fichiers lourds 24.7.2.2 Enregistrer Fichiers transmis à l’utilisateur : output Dans ui : downloadButton(outputId = “myReport”,…) Dans server : diapo p54 24.7.3 Shinydashboard permet de personnalisr la partie ui (rien ne change dans la partie server) Barre de navigation à gauche et une page principale pour les figures 3 parties : header, sidebar, body Dans le body, différentes boites (box) pour les différentes sorties On peut ajouter des onglets dans la sidebar, liée à la partie body Menu avec messages, notif, taches en haut à droite InfoBox, ValueBox (valeurs numériques d’intérets) Personnalisation box (rétractable, ) la hauteur s’adapte au contenu 24.8 Autres packages shinyjs : visibilité d’élmt, activité d’élmt, attributs d’éléments (couleur selon la class), code javascript shinyjs::hidden : cacher un bouton shinyWidgets : bouton switch, sliderText shinyalert : création de boite de messages personnalisée shinyhelper : ajouter de l’aide à une appli (description) plotly : graphiques ggplot2 intéractifs shinysccloaders : spinner (truc qui tourne) pendant le calcul shinyFeeback ou shinyvalidate : avertissement, erreur sur les valeurs saisies Tableau éditable : excelR, DataEditR, shiny-matrix Crosstalk : sélection commune et synchronysée entre plsrs outputs (pour plotly, leaflet, DT) ShinyMobile : plus adaptée à une interface téléphone, ressemble plus à une appli 24.9 Mise en ligne et partage de l’appli 24.9.1 En local : c’est mieux, plus rapide car outils de calcul et on y est seul à travers un package, on peut lancer son appli avec le Addins dans Rstudio ou juste dépot pour l’appli sur github Application Windows sans ouvrir Rstudio, avec un R-portable (lourd) 24.9.2 A distance plsrs utilisateurs sur 1 serveur R/Shiny ShinyProxy : 1 R pour chaque utilisateur Hébergement dédié (Shinyapps.io) : offre gratuite avec des limitations 24.10 Module -&gt; Répéter des outputs pour différents onglets dans ui : création des onglets avec des données spécifiques dans server : callModule() Code du module dans dossier R (pas de sous-dossier), qu’il faudra sourcer fct avec code pour ui : création d’ID unique avec shiny::ns() fct avec code pour server 24.11 Autres sources cours mastering shiny "],["stats-à-ne-plus-chercher.html", "Cours 25 Stats à ne plus chercher 25.1 Inférer des données 25.2 Standardizer (centrer-réduire) 25.3 Régression 25.4 Analyse de variance / déviance 25.5 Analyse de la covariance 25.6 ACP Analyse en Composantes Principales (TP3 d’écologie numérique) 25.7 AFC Analyse Factorielle des Correspondances 25.8 ACM Analyse des Correspondances Multiples 25.9 Analyse mixte/Analyse factorielle multiple 25.10 Corrélations", " Cours 25 Stats à ne plus chercher 25.1 Inférer des données = Remplacer les NA selon les autres variables MICEinf &lt;- mice(data, maxit=100) 100 itérations, methodes par défault Visualiser les valeurs produites MICEinf\\(imp\\)varinf Mettre ces valeurs inférées dans ma base Data_completed &lt;- complete(MICEinf,5) ici j’ai pris la 5eme (m) estimation 25.2 Standardizer (centrer-réduire) Quand standardiser? - Nature différente des variables - Gammes de variations des variables très différentes - Variables dans des unités différentes Uniquement centré : uniquement quand variables de meme unite et de meme gamme de valeurs. as.vector(scale()) ou (var-mean(var))/sd(var) vérifiaction : - round(mean(var),2) la moyenne arrondie du ph doit etre =0 - sd(var) doit etre =1 ou d=as.data.frame(scale(data)) boxplot(d) 25.3 Régression Pour : var explicative quantitative + droite de régression linéaire au sens des moindres carrés : abline(lm(yx)) Pour récupérer les paramètres de la droite : lm(yx)$coefficients La 1ere valeur (Intercept) correspond à l’ordonnée à l’origine la 2nde au coefficient directeur de la droite. linéaire de degré 1 : stat_smooth(method = “lm”, formula = y ~ x) qudratique : 25.4 Analyse de variance / déviance Pour : var explicative qualitatives 25.5 Analyse de la covariance Pour : var explicatives quantitatives et qualitatives 25.6 ACP Analyse en Composantes Principales (TP3 d’écologie numérique) Pour : analyse de plusieurs variables quantitatives ou ordinales - Représentation graphique - Analyse des corrélations - Réduction de dimension L’objectif est de définir de nouvelles variables (composantes principales) comme étant des combinaisons linéaires des variables d’origine, telles qu’elles soient: - non redondantes - Non corrélées(=indépendantes : angle de 90°): - De variances maximales et décroissantes (= sont ordonnées) Etapes : 1) standardiser les données 2) calcul de la matrice de covariance (symétrique) (cor()) 3) calcul des valeurs propres 4) selection des composantes principales 5) Transformation des données dans le nouvel espace 25.6.1 Pour gerer ces donnees manquantes il serait possible de : les retirer (individus ou variables) ou si peu de variables : utiliser PCA() de FactoMineR qui remplace (“impute”) les NA par la valeur moyenne de la variable (approche “grossiere”), ou : la plus juste : utiliser une ACP iterative regularisee avec la fonction imputePCA() du package missMDA, voir video pour plus de details : http://factominer.free.fr/missMDA/PCA-fr.html ACP normee : acp &lt;- ade4::dudi.pca(data, scale=T, center=T, nf = 4, scannf = FALSE) nf : nbr d’axes à garder Uniquement centré : uniquement quand variables de meme unite et de meme gamme de valeurs. barplot(acp\\(eig) # Valeurs propres mean(acp1\\)eig) doit être =1 25.6.2 Calcul du % d’explication de chaque axe (inertie) Quel est le nombre de composantes principales expliquant un minimum de x % de l’information (ou inertie totale). Ce pourcentage d’inertie est calculé en divisant la valeur propre d’un axe par le nombre d’axes possibles (égal au nombre de variables du tableau de départ) pc &lt;- round(acp1$eig/sum(acp1$eig)*100,2)#arrondi a 2 chiffres apres la virgule pc &lt;- as.numeric(as.character(pc)) #passer de character a numerique pc #72.96 22.85 3.67 0.52 # ne retenir que les plus explicatifs (generalement &gt; 10% de variation) (ici les 2 1ers) # % cumules cumsum(pc) # ou data.pca &lt;- princomp(corr_matrix) # l&#39;ACP summary(data.pca) # les statitisques de l&#39;ACP data.pca$loadings[, 1:2] # relation variables-composantes principales 1 et 2 fviz_eig(data.pca, addlabels = TRUE) # eigenvalues plot (scree plot) fviz_pca_var(data.pca, col.var = &quot;black&quot;) # Biplot of the variables fviz_cos2(data.pca, choice = &quot;var&quot;, axes = 1:2) # COS2 plot (contribution of each variable) ou : corrplot(data.pca$cos2, is.corr=FALSE) fviz_pca_var(data.pca, # Biplot combined with cos2 col.var = &quot;cos2&quot;, # ou alpha.var = &quot;cos2&quot; gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE) # Avoid text overlapping fviz_pca_ind(data.pca, # Biplot combined with cos2 col.var = &quot;cos2&quot;, # ou alpha.var = &quot;cos2&quot; gradient.cols = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), repel = TRUE) # Avoid text overlapping fviz_pca_ind(iris.pca, geom.ind = &quot;point&quot;, # show points only (nbut not &quot;text&quot;) col.ind = iris$Species, # color by groups palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), addEllipses = TRUE, # Concentration ellipses legend.title = &quot;Groups&quot; ) # To remove the group mean point, specify the argument mean.point = FALSE. If you want confidence ellipses instead of concentration ellipses, use ellipse.type = “confidence”. 25.6.3 Cercle de correlation (pour une ACP normee) La longueur des flèches indique la part de leur information représentée par les deux axes. L’angle entre deux flèches représente la corrélation qui les lie : angle aigu = positive ; angle droit = nulle ; angle obtus = négative. Une variable est bien représentée si la longueur de son vecteur est proche de 1 (=proche du cercle). Pour les variables bien représentées, les angles indiquent les corrélations entre: Variables et composantes principales : les variables très proches de l’axe sont donc les variables redondantes qui définissent la composante principale qu’on a créé. Variables entre elles (angle petit -&gt; forte corrélation (=COS~1)) (angle proche de 90° -&gt; corrélation nulle) Si les flèches sont trop courtes, les axes ne peuvent pas être interprétés. version simple : pca &lt;- FactoMineR::PCA(data, scale.unit = T,) plot.PCA (pca, choix=“ind”, invisible=“ind.sup”) plot.PCA (pca, choix=“var”, invisible=“ind.sup”) fviz_pca_var(pca, axes = 1:2) Version plus brute : s.corcircle(acp1$co, xax=1, yax=2) $co : column coordinates (variables) = corre des variables sur les composantes factorielles Representation des variables dans le cas d’une ACP centree : s.arrow(acp$co) s.label(acp$li[,1:2], clabel=0.5) representation des individus sur les axes 1 et 2 s.label(acp1$li, label=Species) avec leur nom d’sp Coordonnees des individus sur l’axe 1 (x) en fct de leur valeur pour chaque variable (y) : score(acp) en ordonnees les valeurs des variables (cm) Biplot (attention a l’interpretation) scatter(acp) projection des individus et des var sur le meme graph Les individus les plus éloignés du centre du nuage de points et les flèches les plus longues pour les variables, sont ceux qui sont le mieux représentés par les axes. 25.6.4 Representation d’une variable explicative qualitative : http://www.sthda.com/french/articles/38-methodes-des-composantes-principales-dans-r-guide-pratique/73-acp-analyse-en-composantes-principales-avec-r-l-essentiel/ s.class(acp$li, iris[,5], col=c(1:length(iris[,5]))) L’ellipse est un resume graphique et non un intervalle de confiance. Elle regroupe environ 67% des valeurs si cellipse = 1.5 (valeur par default), ou 95% si cellipse = 2.5 ; ceci si le nuage est un échantillon aléatoire simple d’une loi normale bivariée : voir details https://pbil.univ-lyon1.fr/R/pdf/qr3.pdf Sans ellipses : 67% des données s.class(acp1$li, iris[,5], cellipse=F, col=c(“purple”,“orange”,“green”)) Sans lignes : s.class(acp1$li, iris[,5], cstar=F, col=rainbow(3)) Si un 3eme axe avait ete pertinent a analyser, il est possible de realiser une representation 3D : https://planspace.org/2013/02/03/pca-3d-visualization-and-clustering-in-r/ 25.6.5 Data transformation in new dimensional space This reorientation is done by multiplying the original data by the previously computed eigenvectors. 25.7 AFC Analyse Factorielle des Correspondances Pour : 2 variables qualitatites 25.8 ACM Analyse des Correspondances Multiples Pour : &gt;2 variables qualitatives 25.9 Analyse mixte/Analyse factorielle multiple Pour : variables quantitatives et qualitatives 25.10 Corrélations Pearson ou Spearman ? Pearson : relations lineaires monotones Spearman : pas lineaires monotones et à valeurs extremes simplement Pearson : cor(data) simplement Spearman : cor(data, method = “spearman”) cor.test(var1, var2, method= “spearman”) par indices : by(datatocor, INDICES = colindex, cor) 25.10.1 Draftsman plot pour étudier les variables et leur relation (pour moins de 10 variables) histograms and correlations L’ACP se prête mieux à l’analyse de relations linéaires et de variables ayant une distribution symétrique qu’on peut vérifier avec pairs.panels pairs.panels(iris, method = &quot;spearman&quot;, #coef de correlation de Spearman à droite hist.col = &quot;#00AFBB&quot;, density = TRUE, # affiche la courbe de densite ellipses = T) # show correlation ellipses (interprétable que pour les var quanti) Une base de données uniquement quantitatives et sans NA à tester : baseforcor &lt;- data %&gt;% select(-var quali) %&gt;% na.omit() pas besoin de tester la normalité si n&gt;30. Construire une matrice de corrélation : CorMatrix1 &lt;- round(cor(baseforcor, use = “pairwise.complete.obs”), digits = 2) # matrice de corrélation, avec 2 décimales CorMatrixS &lt;- Hmisc::rcorr(as.matrix(baseforcor)) CorMatrix &lt;- CorMatrixS$r avec les p-values : Pval_corr &lt;- CorMatrixS$P Plot de la matrice de corrélation : http://www.sthda.com/french/wiki/visualiser-une-matrice-de-correlation-par-un-correlogramme corrplot(CorMatrix, method=&quot;circle&quot;, type=&quot;lower&quot;, diag = F, col=brewer.pal(n=8, name=&quot;PuOr&quot;), # &quot;PuBu&quot; addCoef.col = &#39;white&#39;, number.cex = 0.7, is.corr = F, # to range the legend with the values col.lim=c(0, 1), # to limit the legend tl.col=&quot;black&quot;, tl.cex = 1, tl.srt=45, p.mat = Pval_corr) #avec une croix pour les p-value &gt; 0.05 (sig.level = 0.05) "],["version-seulment-corr-0.html", "Cours 26 Version seulment corr &gt;0.5 26.1 Evaluer la performance d’une prédiction 26.2 Processus gaussiens 26.3 Indices de distances et de dissimilarité/Classification ascendante hierarchique CAH/Creation de dendrogrammes/Kmeans (voir TP2 d’ecologie numérique) 26.4 Permanova : anova multivariée", " Cours 26 Version seulment corr &gt;0.5 # inspired by the function of Catherine Williams onlySignCorr &lt;- function(df, file){ corr &lt;- cor(df) #drop perfect correlations corr[corr == 1] &lt;- NA #turn into a 3-column table corr &lt;- as.data.frame(as.table(corr)) #remove the NA values from above corr &lt;- na.omit(corr) #select significant values corr &lt;- subset(corr, abs(Freq) &gt; 0.5) #sort by highest correlation corr &lt;- corr[order(-abs(corr$Freq)),] #turn corr back into matrix in order to plot with corrplot mtx_corr &lt;- reshape2::acast(corr, Var1~Var2, value.var=&quot;Freq&quot;) #plot correlations visually # corrplot(mtx_corr, type=&quot;upper&quot;, is.corr=FALSE, tl.col=&quot;black&quot;, na.label=&quot; &quot;) # file &lt;- &quot;LiDAR_vs_LAI2200&quot; # file &lt;- &quot;LiDAR_Acquisitions&quot; png(paste(&quot;D:/Mes Donnees/PhD/Figures/lidar/Correlations/Correlations/Intensity_1m/Correlation_plot_&quot;,file,&quot;_sup0_5.png&quot;, sep=&quot;&quot;), width = 1000, height = 743, pointsize=20) corrplot(mtx_corr, method=&quot;color&quot;, col=brewer.pal(n=8, name=&quot;PuOr&quot;), type=&quot;upper&quot;, # order=&quot;hclust&quot;, addCoef.col = &quot;white&quot;, # Add coefficient of correlation tl.col=&quot;black&quot;, tl.cex = 0.8, #Text label color and rotation number.cex = 0.55, # values # hide correlation coefficient on the principal diagonal diag=FALSE, na.label=&quot; &quot;) dev.off() } onlySignCorr(DF_cor_LiDARvsLAI2200, &quot;LiDAR_vs_LAI2200&quot;) un autre type : pairs.panels(iris, method = &quot;spearman&quot;, #coef de correlation de Spearman à droite hist.col = &quot;#00AFBB&quot;, density = TRUE, # affiche la courbe de densite ellipses = T # show correlation ellipses (interprétable que pour les var quanti) ) d’autres fcts de corplot : ggcorrplot() 26.1 Evaluer la performance d’une prédiction 26.1.1 Validation croisée (rééchantillonage) = séparer données en jeux d’entraînement et de test : caret::createDataPartition() 26.2 Processus gaussiens généraliser la procédure à une quantité infinie de dimensions kernlab::gausspr() -&gt; écart-type des prédictions, donnant une appréciation de la précision du modèle 26.3 Indices de distances et de dissimilarité/Classification ascendante hierarchique CAH/Creation de dendrogrammes/Kmeans (voir TP2 d’ecologie numérique) 26.4 Permanova : anova multivariée "],["introduction-à-stan.html", "Cours 27 Introduction à Stan 27.1 Pourquoi les stats bayesiennes ? 27.2 Stan program :", " Cours 27 Introduction à Stan library(readr) library(tidyverse) library(rstan) rstan_options(auto_write = TRUE) #option pour ne pas recompiler à chaque fois !!! gain de temps options(mc.cores = parallel::detectCores()) #option pour ajouter des coeurs au calcul library(bayesplot) #visualiser la chaine de Markov library(shinystan) #for interactive stan output visualization library(rstanarm) #for Bayesian automatic regression modelling using stan library(brms) #Bayesian generalized multivariate non-linear multilevel models using stan 27.1 Pourquoi les stats bayesiennes ? on peut exprimer nos croyances/expertises sur les paramètres (prior) prend en compte l’incertitude proprement permet de prendre tout niveau de complexité de modele Theoreme de Bayes * Prior : proba des paramètres apriori * Vraissemblance : proba des données sachant les paramètres * Posterior : distribution des paramètres sachant les données Stan = un language : - entre crochet - un point-virgule à la fin de chaque ligne - // pour commencer un commentaire - &lt;lower=0&gt; : pour borner la variable 27.2 Stan program : Ôuvrir un fichier stan. aucun # n’est toléré !! 3 blocks de commande : data block on déclare : - la taille du jeu de donnée - les differentes variables parameters block On déclare le nom des paramètres et leurs bornes si on le souhaite model block - on déclare le prior (loi que suit le paramètre) (permet d’augmenter la vitesse d’analyse) - sinon prior non-informatif - écriture du modèle (vraissemenblance) Les priors doivent être définis sous sens biologique/écologique GLOPNET &lt;- read_csv(&quot;GLOPNET.csv&quot;, skip=10) LES &lt;- GLOPNET %&gt;% filter(BIOME==&quot;TROP_RF&quot;, GF == &quot;T&quot;) %&gt;% select(Dataset, Species, &quot;log LL&quot;, &quot;log LMA&quot;) %&gt;% na.omit() #le bayesien ne supporte pas les NA LES %&gt;% ggplot(aes(`log LMA`, `log LL`, col=Dataset)) + geom_point() + xlab(&quot;Logarithm of Leaf Mass per Area (LMA)&quot;) + ylab(&quot;Logarythm of Leaf Lifespan (LL)&quot;) Modèle proposé : _log LL ~ N(alpha + beta *log LMA, sigma^2)_ -&gt; régression linéaire à mettre dans un fichier stan data &lt;- list( N = dim(LES) [1], #les noms des var doivent etre les memes que dans le fichier stan logLMA = LES$&quot;log LMA&quot;, logLL = LES$&quot;log LL&quot; ) fit1 &lt;- stan(&quot;stan.stan&quot;, data = data) #LET&#39;S GOOOO! Données injectées, compilation lancée ! Nombre de chaine par défaut mais on peut choisir, 4 c’est le minimum pour interpréter les graphes. thin = période d’amaigrissement = pas de l’itération warmup = periode de chauffe : petite balade aléatoire lp_ = log de la vraisemblance n_eff = nbr d’itérations effectives : qui sont relevantes Rhat doit être égal à 1 (ou 1.1): ça veut dire que l’estimation des paramètres a réussi à converger Ici les chaînes n’ont pas réusi à converger, et ce sont perdues (tres peu de chaînes effectives). on peut rajouter des lignes ggplot pour modifier les plots mcmc_trace(as.array(fit1), #as.array = comme vecteurs facet_args=list(labeller=label_parsed), #pour mettre en lettre greques np = nuts_params(fit1)) # np pour afficher la divergeance MCMC : diagnostic des chaines PPC : comparaison de modèles Il faut une exploration indépendante des paramètres pour trouver le point où ça cohabite = maximum de vraissemblance (voir dessin carnet) #les paramètres doivent être indépendants et former une patate à leur valeur correspondant aux données. mcmc_pairs(as.array(fit1)) On voit que alpha et lp_ sont liés par une relation. On voit sur les graphes des chaines, que alpha veut aller dans le négatif mais ne peut pas car défini sur R+ (loi gamma) Logique biologique : alpha doit être égal à 0 quand LMA est égal à 0 puisque s’il n’y a pas de masse il n’y pas de feuille dont pas de duree de vie !!! Donc on vire alpha du modèle. Nouveau modele : _log LL ~ N( beta *log LMA, sigma^2)_ A mettre dans un fichier stan On peut faire une copie de l’ancien fichier : file &gt; coche le stan file &gt; more &gt; copy fit2 &lt;- stan(&quot;LLLMA.stan&quot;, data = data) mcmc_trace(as.array(fit2), #as.array : comme vecteurs facet_args=list(labeller=label_parsed), np = nuts_params(fit2))#pour mettre en lettre greques mcmc_pairs(as.array(fit2)) Ca fait des belles patates donc c’est bon !!!! Et de beaux histos ! On veut mtn connaitre les distributions des paramètres à posteriori: on veux des paramètres : - différents de 0 (pour une relation entre var expli et var réponse) - et un sigma petit (pour un bon fit) mcmc_areas(as.array(fit2), prob=0.95, pars = c(&quot;beta&quot;, &quot;sigma&quot;)) # pars pour n&#39;afficher que les paramètres que je veux, pas la vraissemblance Posteriors sont normaux et significatifs. mcmc_intervals(as.array(fit2), prob=0.95, pars = c(&quot;beta&quot;, &quot;sigma&quot;)) #vu du dessus launch_shinystan(fit2) Conclusion : - se renseigner sur les formes de lois, de modèle - la définition des lois - centrer-réduire les variables pour faire converger plus vite (meme echelle) - borner les paramètres si possible "],["i-am-a-modeler.html", "Cours 28 I am a modeler 28.1 La modélisation 28.2 Modèle linéaire simple 28.3 Lois de probabilité de la fonction de réponse 28.4 GLM (modèles linéaires généralisés) 28.5 Modèle mixte 28.6 Créer nos prédicteurs selon l’effet que l’on veut tester 28.7 tests 28.8 Estimations des paramètres 28.9 Variable réponse qualitatite", " Cours 28 I am a modeler 28.1 La modélisation On récolte n observations qui sont le résultats de n expériences aléatoires indépendantes. Modélisation : on suppose que les n valeurs sont des réalisations de n variables aléatoires indépendantes et de même loi. Estimation : chercher dans le modèle une loi qui soit le plus proche possible de la loi de notre var réponse = chercher un estimateur de theta0. “Validation” de modèle : on revient en arrière et on tente de vérifier si l’hypothèse de l’étape 2 est raisonnable ((normalité, linéarité, analyse des résidus, test d’adéquation, etc…) Modèle non-paramétrique : univers de dimension infinie Modèle paramétrique : univers de dimension finie 28.2 Modèle linéaire simple Variables quantitatives -&gt; régression Variables qualitatives -&gt; ANOVA (analyse de la variance) Variables mixtes -&gt; ANCOVA (analyse de la covariance) 28.3 Lois de probabilité de la fonction de réponse loi de probabilité = - sa fct de répartition - sa densité Différentes lois : - Variable réponse continue : Normale ou Gamma - Réponse binaire (succès-échecs, présence-absence) : Bernoulli (1 seul tirage), binomiale (plsrs tirages(nbr de succés, proportion)) Comptages (nbr entiers) : Poisson, négative binomiale, Geométrique Durée de survie : Exponentielle Choix (l’adéquation du modèle aux données) : -la déviance normalisée (scaled deviance) : retenir celle qui minimise la déviance D -la statistique du khi-deux de Pearson - AIC, BIC, régression lasso Lorsque le modèle étudié est exact, la déviance normalisée D* ou le khi-deux de Pearson, suit approximativement une loi du khi-deux àn-K degrés de liberté. 28.4 GLM (modèles linéaires généralisés) Dans quel cas ? quand la variable réponse et les variables explicatives ne sont pas définis sur le même univers (intervalle de valeurs). GLM : - Modèle linéaire gaussien : Gaussienne - Régression logistique : Bernouilli (binaire), variable réponse catégorielle, ordinale ou polytomique (modalités) - Log-linéaire : Poisson Une fonction de lien spécifique (= fonction de lien canonique) permet de relier l’espérance μ au paramètre naturel theta (ou canonique) de la loi. En d’autres mots, lier l’espérance de la variable réponse (μ) au prédicteur linéaire construit à partir des variables explicatives. Fcts de lien naturel: - Pour la loi Normale : theta = μ (link=’identity’) - Pour la loi Poisson : theta = log(μ) (link=’log’) - Pour la loi Bernouilli : theta = logit(μ) = log(μ/1-μ) (= logarithme du rapport des chances) (link=’logit’) - Pour la loi Gamma : theta = 1/μ (link=’inverse’) Fcts de liens : - Identité - Logit : est adaptée au cas où μ est comprise entre 0 et1 (par exemple la probabilitéde succès dans une loi binomiale). Approbriée quand les proportions de 0 et de 1 sont équilibrées - Probit : est l’inverse de la fonction de répartition de la loi normale centrée réduite. Approbriée quand les proportions de 0 et de 1 sont équilibrées. - clog–log : Approbriée quand les proportions de 0 et de 1 sont trés déséquilibrées (Hardin and Hilbe (2007)) - Puissance - Logarithme - Gompit( complémentaire log log) [0;1] : logit, probit, clog–log, and log–log Sauf cas (très) particulier, le lien n’est jamais “parfait”. La fonction de lien est inversible. Choix de la fct de lien : le choix de la fonction de lien est libre. Néanmoins choisir la fonction de lien naturel permet d’assurer la convergence de l’algorithme d’estimation utilisé classiquement (algorithme de Newton-Raphson) vers le maximum de vraisemblance. 28.5 Modèle mixte = modèle contenant des effets fixes et des effets aléatoires. Effets fixes = effet d’une variable mesurée, avec des niveaux/groupes qui sont délibérément arrangés par l’expérimentateur, donc bien définie et controlée, sur une var réponse. Effets aléatoires = effet sur la structure de l’échantillon, dont les niveaux sont possibles. Dans l’étude de ce type d’effet on ne s’intéresse pas à l’effet qu’a chacun des groupes mais à la variabilité totale qu’ils apportent à la var réponse. 28.6 Créer nos prédicteurs selon l’effet que l’on veut tester interaction entre 2 variables : X3= X1*X2 (multiplication de var) effet non linéaire : X4 = X1^2 (exposant sur une variable) 28.7 tests Le test t permet de tester l’hypothèse H0 pour chaque variable. Le test de Fisher permet de tester plusieurs paramètres simultanément. Le test de Fisher a plus de sens dans le cas d’une ANOVA car il considère la variable explicative dans son ensemble et non modalité par modalité. 28.8 Estimations des paramètres Par minimisation des moindres carrés ou maximum de vraissemblance Résidus (epsilon) = Y - estimation d’Y (y chapeau) 28.9 Variable réponse qualitatite Chercher à expliquer Y par X revient à chercher de l’information sur la loi de probabilité de Y sachant X. Si on peut, rendre la variable binaire (0,1) -&gt; Bernoulli -&gt; Régression logistique "],["bayesian-stats-cours.html", "Cours 29 Bayesian stats cours 29.1 Grid approximation (to define posterior) :", " Cours 29 Bayesian stats cours design the model (data story) condition on the data (update the model) evaluate the model (critique) ex : 9 times dta : W (water) or L (land) p = proba de W proba de L = 1-p prior : information before the data (p in [0;1]) posterior : update info of each value of p conditional on data chaque postérior est le prior du prochain posterior plus on a de données plus il est aisé d’avoir un résulat précis Define generative relations between the variables W, L, W p * (1-p) *p = p2(1-p)1 : relative number to see W Vraissemblance : 29.1 Grid approximation (to define posterior) : posterior proba = standardizez product of proba of the data and prior proba standardisé : add up all the products and divide by this sum grid approximation uses finite grid of parameter values instead of continuous space too expensive with more yhan a few parameters Sampling from the posterior Intervals : how much mass Percentile intervals (PI): equal area in each tail Hightest posterior density intervals (HPDI) : narrowest interval containing mass Mean nearly always more sensible than the mode Model : "],["bayesian-and-stan-langage.html", "Cours 30 Bayesian and Stan langage 30.1 Règles 30.2 Les données 30.3 Definitions 30.4 Loi Poisson (positif &amp; discret) (par exemble un abondance) 30.5 Loi Bernouilli-logistique (Blogit) (réponse binaire) 30.6 Tools 30.7 Rapidité 30.8 Non-identifiabilité", " Cours 30 Bayesian and Stan langage 30.1 Règles Ne pas comparer des effets de modèles de var réponses diffentes entre eux Ne pas comparer des modèles utilisant des données différentes 30.2 Les données Toutes les variables doivent être mises sous forme numeric Regarder la distribution de nos variables pour identifier la loi qu’elles suivent et les transformations possibles à leur appliquer. Standardiser pour mettre toutes les variables échelles, ainsi faciliter la lecture et comparaison de leurs effets (meme dimension) : cas d’un modèle explicatif. Dans le cas d’un modèle prédictif on veut garder les dimensions de chaque var. Si on veut que les covarariants soient des probabilités : on les borne [0;1] Réduire la taille des données en exploratoire, et ne faire tourner l’ensemble des données lorsque le modèle a été validé -&gt; gain de rapidité 30.3 Definitions theta0 : c’est l’intercept du modèle. C’est dans certains cas l’esperance predite de la var réponse, c’est le témoin. A regarder pour juger de l’importance des autres paramètres. Vraissemblance (totale): sum de toutes les vraissemblances particulières (=pour chaque observation). Vraissemblance totale = nbr d’obs si vraissemblance parfaite du modèle. “la petite montagne que forme la marche aléatoire pour trouver LA valeur de chaque paramètre” priors : Non informatif par défaut : ~ \\(Gamma\\) quand modèle défini sur \\(R+\\), ~ \\(N\\) quand modèle défini sur \\(R\\) On n’utilise pas les données pour définir les priors mais on peut pour borner les paramètres. Il peut être nécessaire de borner les paramètres lorsqu’il n’y a pas d’effet et que la marche aléatoire se perd. Variance (\\(\\sigma\\)) = variance de l’ensemble des erreurs du modèle Erreur du modèle = Var réponse/esperance du modèle 30.4 Loi Poisson (positif &amp; discret) (par exemble un abondance) Var reponse suit une loi \\(P\\) de paramètre \\(\\lambda\\) le paramètre \\(\\lambda\\) suit une loi exp dans laquelle on met les covaraints et leur paramètres pour les faire passer de \\(R\\) à \\(R+\\) nécessaire à \\(\\lambda\\) pour pouvoir comparer les paramètres les faire suivre une loi \\(N\\) Hyperlois = faire suivre une loi à un ensemble de paramètres Qd modele trés chargé en paramètres (vecteurs de parmètres) (pas 1 valeurs mais plsrs) on généralise ceux ayant leur variable associée en commun en 1 (emboités) l’effet nul (theta0) n’a pas besoin d’etre emboité pour etre comparé aux autres paramètres Interet : faciliter la convergence et faire suivre le meme chemin à tout ceux qui doivent le suivre 30.5 Loi Bernouilli-logistique (Blogit) (réponse binaire) logit (=fct de lien) car il faut linéariser les facteurs 30.6 Tools Comparer prior et posterior : ppc_dens_overlay() Validation de modèle selon la capacité de prédiction : loo_compare(loo(mod1), loo(mod2)) %&gt;% kable() Il calule des différences donc le meilleur est à 0. Il faut avoir la valeur la plus haute. Dans le stan file : generated quantities { vector[I] log_lik ; ## vraisemblance pour chaque obs vector[I] prediction ; for(i in 1:I){ log_lik[i] = fct_lpmf(y[i] | theta[i]) ; ## pour loo. ##lpmf -&gt; masse de proba (cas var discrète). ...loi de densité (cas var continue) prediction[i] = fct_rng(theta[i]) ; ## pour ppc_dens_overlay(). rng -&gt; génération de } 30.7 Rapidité Réduire la taille des données 2 chaines et une 100aine d’itérations Ne pas caluler certains paramètres : include = F, pars = “theta” l’argument save_warmup = F (ne pas renvoyer la période de chauffe) 30.8 Non-identifiabilité = vraisemblance reste cste malgrè changement des paramètres, incapcité à donner une valeur aux paramètres (variance trop grande) Symptomes : - chaines ne semélangent pas - corrélations des parmètres En fréquentiste pour pallier ça on implémente des contraintes. Cause : prior non informatif Solutions: centrer les variables (aide les chaînes, et enlève le prblm de corrélation entre param) tirer des paramètres de manières simultanées (par block) (Miltonien) plutot que l’un après l’autre (Metropolis Hastings) sum - to zero contransints : sum de la variance des random effects = 0 Raftery diagnostic : pour déterminer le nbr d’itérations nécessaires reparametrisation by sweeping : ac matrice de covariance post-sweeping of random effects : on écarte les paramètres non identifiables, on ne les interprète pas. on redéfinit l’intercept en prenant l’intercept + variance du modèle "],["inla-inlabru-statistiques-bayésiennes-spatialisées.html", "Cours 31 Inla-Inlabru statistiques bayésiennes spatialisées 31.1 Install INLA R package 31.2 Other packages 31.3 Load packages", " Cours 31 Inla-Inlabru statistiques bayésiennes spatialisées Formateur : julien.papaix@inrae.fr, Thomas Opitz 31.1 Install INLA R package https://www.r-inla.org/download-install install.packages(&quot;INLA&quot;, repos = c(getOption(&quot;repos&quot;), INLA=&quot;https://inla.r-inla-download.org/R/stable&quot;), dep=TRUE) 31.2 Other packages install.packages(c(&#39;inlabru&#39;, &#39;raster&#39;, &#39;tidyverse&#39;, &#39;sf&#39;, &#39;ggplot2&#39;, &#39;ggpolypath&#39;, &#39;rgeos&#39;, &#39;viridis&#39;)) 31.3 Load packages library(INLA);library(inlabru); library(raster);library(tidyverse);library(sf) library(ggplot2);library(ggpolypath);library(rgeos);library(viridis) "],["cours.html", "Cours 32 Cours 32.1 Approche bayésienne 32.2 inlabru", " Cours 32 Cours Les grands domaines des stats spatiales : - données continues irrélulièrement espacées (géostatistiques) - Données sur lattice (pas positions exactes, mais connaissances des relations de voisinages, mesures des corrélations) - processus ponctuels : présences seules dans l’espace et dans le temps Données géostatistiques : ex : variables environnmentales Données sur réseaux : ex : données de pop, épidémio Processus ponctuels ou d’objets : ex : reapartition d’sp, cas de maladie Spécificités/difficultés : - données non-indépendantes - pas de realation d’ordre - vraissemblance adaptée ? Géostatistique classique : - échantillonage : données suffisantes - analyse structurale ou variographique - interpolation - modélisation Champs aléatoires : - décrire la var spat - interpoler (cartographier) - evaluer l’erreur d’interpolation Modèle : un champ aléatoire Z(x) ou …. Stationnarité : loi invariante par translation : si la distance est la meme la loi est la meme dépend uniquement de la distance Ergodicité : inférer les paramètres (moments) amats indépendants h : la distance covariance : décrit la structuration et les corrélations spatialess Effet aléatoire pur : bruit blanc spatial non corrélé dans l’espace Modèle de covariance exponentiel : changement abrupte de valeur Portée : point à partir duquel la corrélation est nulle -&gt; portée faible = petits amats Modèle de covariance gaussien : plus lisse que l’exponentiel, moins brutal Modèle de covariance gaussien avec effet pépite : ajout de variabilité locale (bruit) 32.1 Approche bayésienne Vraissembance : adéquation entre la distribution observée et une loi de proba supposée. La distribution des données sachant les paramètres. le paramètre inconnu theta est une var aléatoire au meme titre que les observations Outis : INLA (Integrated Nested Laplace Approximations) Modèle hiérarchique : différentes couches : - processus d’observation (facon dont j’observe les données (ex: comptage = Poisson)) - processus biologique (description du processus) - paramètres - Hyper-paramètres (qui determinent la distribution des paramètres) Ajouter du spatial : prendre un effet aléatoire ai (résiduel, non expliqué par les covariables qui ne sera plus indépendants et leur corrélation dépend de leur distance dans l’espace. suit une loi normale multivariée. ce a permet de prendre en considaration ce qu’on n’explique pas avec les covariables en raison de covariables inconnues ou de dynamique de pop (colonisation) -&gt; effets aléatoires spatiaux, “modèles mixtes” -&gt; Variabilité spatiale résiduelle L’effet de pépite pourra être dispaché et expliqué par des effets fixes espèces ou autre. On peut coupler plusieurs jeux d’observations : - 1 sp mais différents types d’observations - 2 sp - 1 sp focale (“target”) + espèces de fonds (“target background”) -&gt; régression multi-réponse, effets aléatoires partagés Effort d’observation variable : - données protocolées (-&gt; présences-absences) (effort connu) - données opportunistes (-&gt; présence seule) (effort inconnu) : pas de protocole d’observation, pas de design, biais d’échantillonage car effort d’échantillonage irrégulier. On souhaite à retrouver les chanps de l’effort de l’abondance réelle Présence seule - Différentes granularités spatiales (de la + à la - fine): - coordonnées géographiques -&gt; semis de points -&gt; processus ponctuel * fonction d’intensité (lamda): nombre d’occurences * modèle classique : proessus de Poisson (indépendance des occurences) comptage dans des unités spatiales (dans une grille) Présence-absence dans les unités spatiales -&gt; Bernoulli (1 = au moins 1 occurence) -&gt; perte d’information La taille de la grille va influencer la précision et la perte de l’information. Les processus ponctuels : intensité (lamda(s)): nombre d’occurences moyen par unité spatiale en s. Si on veut un champ aléatoire : Processus de Cox : si lambda(s) est un champ aléatoire : - variabilité stochastique pour représenter les covaraibles inconnues Processus de Cox log-gaussien log(lamda(s)) est un champ gaussien Modèles de régression : - un paramètre clé à estimer relié un prédicteur linéaire mui via une fonction de lien mui intègre les covariables et les effets aléatoires (espace, observateur, année) si on fait de la présence -absence on prendra le cloglog (lien complémentaire) (le lien log simple n’est pas bien adapté) Wo : associé à l’effort d’échantillonage, intervient dans les modèles opportunistes et pas dans les modèle protocolé Représentation des champs spatiaux : - dicrétisation : construction d’un vecteur gaussien, formation d’un maille à noeuds portant les valeurs - interpolation entre ces points via des fcts de base, triangulation de l’espace (la pyramide sur le schéma) - calculs numériques allégés via des représentations de champs gaussiens markoviens (matrices de précision creuses grace à l’approche SPDE) Implémentation via les éléments finis (= fonctions “pyramides”) SPDE (stochastic partial differential equation) : -&gt; solution approximative -&gt; Matrice de précision creuse Q -&gt; calculs matriciels rapides quand m (= le nbr de noeuds) est grand https://becarioprecario.bitbucket.io/spde-gitbook/ Triangulation (“mesh”) pour l’approche SPDE - maillage relativement dense dans le domaine d’étude D (zone intérieure) - possibilité d’augmenter le maillage dans les zones de forte occurence - zone d’extension (au maillage moins fin) pour un bord extérieur de D afin d’éviter les effets de bords L’implémentation R-INLA classique : - estimation des modèles bayésiens hiérarchiques basés sur les processus latent gaussiens - un cadre adapté aux processus de Cox log-gaussiens - utilisation des approximations de Laplace (déterministes pour calculer les lois a posteriori (effets fixes champs W, hyperpaamères)) - penalized-complexity (PC) priors pour les hyperparamètres 32.2 inlabru intéret pour les études en écologie syntaxe plus intuive qu’INLA fcts pour les processus de Cox log-gaussiens estimations et prédictions a posteriori plus faciles à coder et plus de possibilités Sorties standards : - moyenne, médiane, écarts-types, quatiles a posteriori - … - bonne intégration des objets spatiaux de type sp - très complet pour les “présences seules” avec les processus cox log-gaussiens Données de présence seule massives (car sciences participatives) - biais d’échantillonage si on veut utiliser ces donées pour définir la niche des espèces. Technique de correction : target-group background : aproximer l’effort d’éch avec les points d’un groupe cible d’sp (.qmd, une extention du rmd, plus jolie pour afficher, et compatibles avec d’autres langages) créer une mesh Matérn covariance : LGCP : W0 : paramètre partagé de l’effort d’échantillonage offset : terme multiplicatif sur un effet pour montrer un effort d’échantillonage Le spatio-temporel : la dimension temporelle sera portée par le W, et sera à inclure dans la matrice de Matern sur le parmètre rho un exemble en spatio-temporel avec inlabru : https://inlabru-org.github.io/inlabru/articles/web/2d_lgcp_spatiotemporal.html "],["conseils-de-rédaction-scientifique.html", "Cours 33 Conseils de rédaction scientifique 33.1 Conseils généraux: 33.2 Pq écrire ? / Pq publier ? 33.3 Les barrières pour écrire et publier : 33.4 Que veulent les rédacteurs ? 33.5 Fonctionnement d’une revue : 33.6 Introduction (3 paragraphes) (250 mots) (¼ des refs) 33.7 Méthodes (pour que ce soit reproductible) () 33.8 Résultats (pas de plan par défaut, voir les consignes de la revue) 33.9 Discussion on discute le résultat tout le long (10-20h de travail 33.10 Annexes 33.11 Les auteurs (les 4 pts validé, international) 33.12 Références : 33.13 Titre de l’article : 33.14 science ouverte : diffusion des produits scientifiques (résultats, données, écrits et code source) 33.15 Data availability: 33.16 Résumé : 33.17 Choix d’une revue (Coopist, “journal selector”) 33.18 Remerciement : 33.19 CRediT/contributeurs : 33.20 Conflit d’intérêt (relations et activités qui ont pu influencer) : 33.21 Gestion du temps 33.22 Gestion des coauteurs : 33.23 Ecriture (qqsoit la langue) : 33.24 Embellissement de l’article (pas bien) 33.25 Lettre de motivation de soumission (pas tjrs demandée) : 33.26 Au Cirad", " Cours 33 Conseils de rédaction scientifique https://docs.google.com/document/d/1Ny_bmW094guCzg8x11Td7pAbqv9EIwCB6iUyIm4WXUY/edit?usp=sharing Cours par : Hervé Maisonneuve (médecin) lire la BD humoristique “carnet de thèse” 33.1 Conseils généraux: 1 idée/phrase/paragraphe 1 phrase d’intro au début de chaque paragraphe donnant l’info du paragraphe Eviter la répétition de contenu -&gt; plan à revoir Les articles ne font pas des sous-sous-sous parties -&gt; simplifier le plan il faut pas trop attendre après l’écriture pour soumettre car la littérature elle avance une recherche non publiée n’existe pas n’écrivez jamais sans savoir vers quelle revue on se dirige la réanalyse…prend du temps citer plutôt la 1ère étude faire d’abord tjrs lire par un naïf avant soumission restez focus dans le sujet dans la rédaction les chiffres en toutes lettres de 0 à 10 puis en numérique 33.2 Pq écrire ? / Pq publier ? partage valoriser son labo, se donner des “points” -&gt; poursuite de carrière +avancer dans la science former les autres et se former pour faire évaluer son travail 33.3 Les barrières pour écrire et publier : éviter le plagiat mobiliser les sources, la citation expliquer clairement et simplement proposer une mise en perspective l’anglais la procrastination collaboration avec co-auteurs 33.4 Que veulent les rédacteurs ? innovation/originalité dans certaines disciplines la réplication d’une recherche est valorisée clair/simple/concis 50-70% de ce qui est publié n’est pas reproductible 33.5 Fonctionnement d’une revue : peer-review, aveugle ou non (60% simple aveugle (auteur connu reviewer inconnu), ouvert tt le monde connu) Impact factor basées sur les citations rédacteur en chef : pouvoir de régulation relecteur récupère des idées, régule lui aussi propriétaire de la revue a des intérêts économique le lecteur n’a aucun pouvoir car il ne paye pas et on ne lui demande pas son avis 33.5.1 Différents types d’articles : éditorial article revue de la litt thèses, mémoires notes techniques, méthodes correspondances livres, chapitres Article scientifique : - réponse à une question - modèle IMRaD : intro, mat, results, discu - tout au passé - voie active - 4000 mots environ sans résumé et ref - 7-8 pages - 30-40 citations 33.6 Introduction (3 paragraphes) (250 mots) (¼ des refs) Etat de l’art (connu) : contextualiser le sujet du plus général (pas des faits ridiculement trop large et surconnu) au plus ciblé gaps (inconnu) : les manques dans la littérature + hypothèses La question (les hypothèses) Réponse à la question dans certaines disciplines au passé, parfois présent les refs : pas des tartines seulement les plus originales, importantes, pas tt le monde 25-30% des refs ne contiennent pas ce qu’on leur fait dire 33.7 Méthodes (pour que ce soit reproductible) () Sélection (nos choix éclairés) : design, temporalité, lieu, sélection des caractéristiques de l’objet d’étude Data availability : accès aux données Interventions/manipulations/observations : ce qu’on y a appliqué description de l’outil (marque, n°série, année) méthode (durée, temp, etc) investigateurs et la lecture : qui a manipé, combien de personnes, (juniors/seniors) Evaluation : quelles vars évaluées les tests préliminaires peuvent y être mis si courts sinon résultats comment : les stats considération éthique : y avait t-il des régulations le protocole peut se mettre en annexe 33.8 Résultats (pas de plan par défaut, voir les consignes de la revue) au passé pas de ref présentation des résultats originaux, les autres en annexes tableau en priorité (données numériques) (ind en lignes, vars en col en général) figures (certaines en annexe si trop long) la figure ou le tab doit se suffir à elle même avec sa légende texte parle de l’info principale du tab ou de la figure, ne doit pas expliquer les fig/tab 1 résultat n’est présenté qu’1 fois (tableau ou figure) présenter nos figures à collègue naïf pour vérifier leur indépendance la lecture commence en haut à gauche prendre en compte ce que l’oeil doit comparer unités des axes éviter les 3 dimensions, les camemberts, la perspective la double unité est moins lisible mettre des délimitations pour indiquer où regarder sur les photos éviter les noms d’axes verticaux qd on n’a pas de 0 il faut couper l’axe enlever le vide inutile utiliser la police de la revue Les figures : couleurs : couleurs de la revue, penser aux daltoniens et impression noir et blanc (symboles) 33.9 Discussion on discute le résultat tout le long (10-20h de travail passé/présent/conditionnel pas de future pas de rappel de la question répondre à la question (1 paragraphe) (on répète le résultat) forces puis faiblesses du travail (dans chacune des parties) comparaison avec la littérature signification de ce travail (optionnel) (changement de pratiques, nouvelles hypothèses, implications générales) perspectives spin : abus de langage en disant que qqchose marche pas mais que ça aurait marché si… enduit le lecteur en erreur. Les résultats positifs en plus de ça plus représentés dans la littérature que les résultats négatifs. 33.10 Annexes 33.11 Les auteurs (les 4 pts validé, international) contribution à la conception ou aux méthodes, ou à l’acquisition, l’analyse ou interprétation des résultats a rédigé ou participé à la révision critique approbation pour la dernière version engagement à assumer l’imputabilité (droits) pour tous les aspects de la recherche Ordre des auteurs (pas de recommandations, mais de pratiques, négociation entre auteurs) : - celui qui a dirigé l’étude - entre les deux : ordre d’implication - le dernier : superviseur général Si contribution égale : on le spécifie et l’ordre des auteurs on peut tirer au sort Les revues n’exigent rien mais : - auteurs cadeaux/honoraires : n’ont pas participé mais mis en auteurs - auteurs fantômes : ont participé mais ne sont pas mis en auteurs 40% des conflits dans les équipes est dû à l’ordre des auteurs. Remerciements : demander l’autorisation consentement 33.12 Références : tt ce qu’on utilise doit être cité règles selon la revue logiciels : zotero (très bien pour une thèse), mendeley, endnote (payant mais plus professionel) citation alphabétique (en ttes lettres) ou numérique (ac des numéros) communication personnelle cité avec son nom et avec son accord “données non publiées” ça n’existe plus 33.13 Titre de l’article : court (10 mots max) l’info essentiel de l’article (sujet, rslt principal), le message types, informatif, neutres, marketing (jeux de mots) abréviations que si connues lieu mit s’il est important dans le résulat en vu de la littérature (peut faire perdre des lecteurs) humour : pas le même selon les cultures question ? a voir sous-titre si besoin effectif si original pour la thématique Running title : titre revient au début de chaque page Mots clefs : https://methodsblog.com/2015/12/18/seo/ - bien choisis pour les moteurs de recherche - pas figurant dans le titre 33.14 science ouverte : diffusion des produits scientifiques (résultats, données, écrits et code source) principe FAIR : Findable Accessible Interoperable Reusable Dataverse Cirad : dépot de données institutionnel Si les financements sont publiques, l’étude doit être ouverte voie verte : arrêter les revues et publier sur des serveurs institutionnels voie dorée : l’institution de l’auteur paye pour publier (2000-5000e/article), pour le lecteur c’est gratuit (=open accès) voie diamant : “gratuit pour tt le monde”, c’est la revue de l’institution et c’est elle donc qui paye le personnel. Preprint : pré publication soumis ou non, relus par les pairs ou non, ou rejeté après soumission (HAL dépôt de preprint). Risques d’erreurs et de leur propagation, certaines plateformes proposent des corrections et le statut de l’article. On peut citer un préprint en précisant. PubPeer : critiques par les pairs sur des articles acceptés, en téléchargeant le plugin on est informé des critiques faites sur les articles qu’on télécharge Revues prédatrices (surtout indiennes) (14 000) : but de faire de l’argent en proposant des prix plus bas (200-400e) mais un travail de moindre qualité (2-3 semaines), porte un nom ressemblant à celui d’une revue connue, procède par des propositions par mail (ne pas se désabonner pcq c’est pire, juste les jeter), les procédures ne sont pas transparentes. Risque pour la réputation Orcid : open researcher and contributor iD = syst d’identification pour les chercheurs, prévient des cas de changements de noms, d’homonymie, permet de s’identifier rapidement et tout rassembler, fait son CV 33.15 Data availability: accès aux données nécessaires à la reproductibilité, fournir les images originales déposer dans des référentiels de données publics selon thématique se conformer aux normes/fichiers d’info complémentaires, instructions aux auteurs mention “demande des données aux auteurs” ils ne les donne svt pas il est bien de donner accès sous motif d’utilisation 33.16 Résumé : 250-400 mots max à rédiger après l’article clarté et mots clefs objectifs, question, méthodes, principaux résultats (chiffrés), réponse à la question (pas discussion) faire du copié-collé de phrases fortes (hilights) de l’article (c’est pas du plagiat) dernière phrase de l’intro 1ère phrase de chaque paragraphe … qqfois des “graphical abstract” (schéma bilan) (bien pour publier sur Twitter) ou sous forme de podcast mtn ! 33.17 Choix d’une revue (Coopist, “journal selector”) A remplir ! 33.18 Remerciement : les financeurs relecture par un anglophone non émotionnel (thèse oui) 33.19 CRediT/contributeurs : contributions des auteurs et des non auteurs qui remplissent 1 des points (collecte, analyse, recherche financements, consulté pour la méthodo, ressource matériel, supervision, rédac, relecture) 33.20 Conflit d’intérêt (relations et activités qui ont pu influencer) : préciser les discordances entre chercheurs préciser ses opinions dans le cas où ça pourrait jouer (décisionnaire, actif, dons économiques) préciser qui a financé préciser si certains relecteurs sont contre indiqués c’est le lecteur qui se fait un avis sur les liens exposés 33.21 Gestion du temps combien d’heures pour écrire un manuscrit (heures étalées sur x mois) comment optimiser, s’organiser : où écrire : endroit où on y est tranquil, isolé recherche documentaire doit être faite avant et il faut arrêter temps social important pour s’ouvrir Quand écrire : même si la recherche n’est pas terminée ux moments de la journée où on est le plus productif régulièrement (un peu ts les jours) sinon on perd du temps à s’y replonger et ça démotive (dont interruption continue) . Les temps de repos sont essentiels. Ne pas attendre d’avoir une journée de libre, sinon on n’écrit jamais. 25min minimum, 3h maximum (relecture, plans, relire notes, écriture) s’arrêter avec des idées pour la suite (qu’on note) pour mieux s’y remettre effet Zeigernik : on se souvient mieux de ce qui est inachevé que de ce qu’on a terminé, le cerveau continu à y travailler conseils : discuter régulièrement avec les coauteurs pour alimenter la réflexion Procrastination, 6 conseils : 1) pensez au futur, au calendrier, imaginez l’article accepté, etc… 2) apprendre à gérer vos émotions et les sentiments négatifs 3) Diminuer les distractions… 4) Responsabilité : revoir et préciser les objectifs 5) Demander de l’aide, car écrire est assez facile, quand on a appris… 6) Renforcez votre volonté.… de l’entraînement et des ressources et ce sera facile 33.22 Gestion des coauteurs : constitution équipe - transparence - choix de l’ordre - plan des gestion des données - stratégie de rédaction - prendre en compte diversité culturelle et linguistique - éthique - avoir une date limite - écrire sur un drive pour écrire sur seule version 33.23 Ecriture (qqsoit la langue) : phrase simple courte (1 action par phrase) chaque mot doit être utile (enlever : kind of, really, basically, practically, actually, virtually etc) 15-20 mots/phrase pas de style position forte : info principale : au début de la phrase, au début du paragraphe (pas de suspens) forme active utiliser le même mot à chaque fois pour caractériser qqchose (les répétitions c’est ok) les mots scientifiques doivent être utilisés que dans leur cadre pas de double négatifs précision : mots, chiffres, totaux, faits, opinions, ref pas d’expressions émotionnelle peu d’adverbe (gérondif) (fr : en verbe-ant; en : verb-ing) ne pas utiliser des mots indéfinis : it/these/that éviter les démonstratifs (ces, cette etc) éviter de commencer une phrase par un acronyme pas faire de doubles parenthèses, mettre un ; entre les deux élements pas parler de “good” ou “bad” mais de “quality of” 33.23.1 En englais pas d’appositions en début de phrase you cannot “explicit something”, you can “make something explicit” 33.24 Embellissement de l’article (pas bien) sélection de ce qu’ils mettents dans l’article p-value arrangée images modifiées données enlevées salami : segmentation d’articles en plusieurs petits empêche la reproductibilité 33.25 Lettre de motivation de soumission (pas tjrs demandée) : 1 seule page regarder si modèle pour la revue accompagne l’article pour attirer l’attention de la revue mise en évidence des découvertes et résultats importants, des objectifs communs avec la revue on peut dire pq on a choisi leur revue, qu’on l’a connait ne pas parler négativement d’un concurrent ou de politique, on ne parle que de son étude italique nom de la revue et des espèces éviter de parler de notre matériel de travail le manuscrit doit être original, pas déjà publié, pas de conflits d’intérêt liste des relecteurs si demandée, ou ceux qui ne devraient pas relire notre manuscrit 33.26 Au Cirad aide à la publication à la DIST au Cirad “portail du libre accès” du site cirad Agritrop : archives ouvertes (base de données) en ligne du CIRAD Mme Fovet-Rabot : s’occupe de l’archivage au cirad Montpellier "],["formulation-de-phrases-élégantes.html", "Cours 34 Formulation de phrases élégantes", " Cours 34 Formulation de phrases élégantes Dans ce sens, des études montrent que… (ref "],["distinction-britaniqueamericain.html", "Cours 35 Distinction britanique/americain 35.1 -our (UK) ou -or (US) 35.2 -re (UK) ou -er (US) 35.3 -ise (UK) ou -ize (US) 35.4 -nce (UK) ou -nse (US) 35.5 -ge (UK) ou -g (US) 35.6 -ae (UK) ou -e (US) 35.7 -ogue (UK) ou -og (US) 35.8 -que (UK) ou -ck (US) 35.9 voyelle + L 35.10 Past simple 35.11 Get 35.12 Prépositions 35.13 have got (UK) vs have (US) 35.14 as (UK)/like (US) 35.15 Les dates 35.16 Les contractions", " Cours 35 Distinction britanique/americain Règle générale : Le britanique a gardé l’orthograohe originale des mots alors que l’américain préfère une orthogrophe plus proche de la prononciation. 35.1 -our (UK) ou -or (US) Souvent les mots terminant par -eur en français ex: colour/color, favour/favor, humour/humor, labour/labor, favourite/favorite, neighbour/neighbor 35.2 -re (UK) ou -er (US) Souvent les mots terminant par -re en français ex: centre/center, kilometre/kilometer, metre/meter 35.3 -ise (UK) ou -ize (US) Généralement des verbes ex: organise/organize, apologise/apologize, recognise/recognize 35.4 -nce (UK) ou -nse (US) ex: defence/defense, licence/license, offence/offense 35.5 -ge (UK) ou -g (US) ex: judgement/judgment, ageing/aging, arguement/argument 35.6 -ae (UK) ou -e (US) ex: encyclopeadia/encyclopedia, orthopeadics/orthopedics, aeon/eon, mediaeval/medieval, manoeuvre/maneuver 35.7 -ogue (UK) ou -og (US) ex: analogue/analog, catalogue/catalog, dialogue/dialog 35.8 -que (UK) ou -ck (US) ex: cheque/check, chequer/checker 35.9 voyelle + L Verbes qui se terminent par une voyelle suivie de la lettre L à l’infinitif, dans leurs formes plus longues (conjuguées ou substantivées), les Anglais doublent le L, en américain on double le L si l’accent porte sur la 2e syllabe ex: travelled/traveled, travelling/traveling, traveller/traveler Attention ! To excel et to propel (et leurs dérivés) prennent toujours deux L. 35.10 Past simple Certains verbes sont réguliers en américain et prennent un -ed au passé simple, alors qu”en britanique ils sont irréguliers et prennent un -t. ex: dreamt/dreamed, learnt/learned, spelt/spelled, burnt/burned 35.11 Get UK: get, got, got US: get, got, gotten 35.12 Prépositions Le temps (at/on) : UK: at the weekend US: on the weekend 35.13 have got (UK) vs have (US) UK: I have got US: I have 35.14 as (UK)/like (US) UK: I felt as if I was talking to a stranger. As I told you. US: I felt like I was talking to a stranger. Like I told you. 35.15 Les dates UK: le jour en 1er US: le mois en 1er ex: UK: 4th April à l’écrit, généralement prononcé the fourth of April US: April 4th prononcé tel quel, ou parfois April the fourth. 35.16 Les contractions Certaines formes contractées sont beaucoup plus fréquentes en anglais américain. Elles sont très familières et ne s’emploient que dans la langue parlée (et dans les chansons). Mais il faut savoir les reconnaitre. ex (US): wanna = want to gonna = going to gotta = have got to (ou have to) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
